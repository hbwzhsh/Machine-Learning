### Random Machine Learning Projects

Note: Projects were completed for personal growth and each has a varying level of clear documentation and algorithmic efficiency.

#### Project descriptions: 
- dino_runner_yolo: YOLO model implementation to detect whether a user's palm is open or closed (based on personal data), and separate driver program that feeds commands to play the Chrome dinosaur game using Selenium.
- trigger_word_detector: Unidirectional multilayered RNN model to detect when a trigger word is said (based on personal data and heavy data synthesis), and separate driver program using multithreading to detect utterances on the fly.
- face_recognition: Implemented a facial recognition model based on FaceNet.
- neural_style_transfer: Implemented a model based off the paper - A Neural Algorithm of Artistic Style. ([Video](https://www.linkedin.com/posts/nathaniel-andre_computervision-deeplearning-activity-6484430196412944384-Zjj3))
- deep_learning_for_nlp:
  1. ByteNet Character-level translation model.
  2. Implementation of a RNN with an attention mechanism for the task of date normalization.
- recommender_systems: Collaborative filtering implementation.
- language_models: 
  1. Character level RNN for Goethe's Wilhelm Meister.
  2. Word level CNN with Gated Linear Units for Melville's Moby-Dick.
- anomaly_detection: Gaussian anomaly detection w/ multivariate and univariate implementations.
- object_detection:
  1. googLeNet_model: Implemented model based on GoogLeNet.
  2. resnet_model: Implemented resnet-50 model in Keras and Tensorflow.
- clustering: 
  1. K-means clustering implementation.
  2. DBSCAN density clustering implementation.
- genetic_algorithm:
  1. Simple instance of a genetic algorithm to find parameters that maximize a simple objective function.
  2. Simple genetic algorithm to learn how to play an implementation of the snake game.
- autoencoders: 
  1. DNN/CNN auto-encoders for denoising with MNIST.
  2. variational auto-encoder for generation with MNIST.
- simple_models:
  1. svm: SVM implementation using simple gradient descent, for linear and gaussian kernels.
  2. dense_neural_net: Simple neural network with hidden layers and sigmoid/relu activation, xavier initialization, and adam optimization implementations.
  3. linear_regression: Linear regression implementation using gradient descent.
  4. logistic_regression: Logistic regression implementation using gradient descent.
  5. naive_bayes: Bayesian classification model based on gaussian likelihood.