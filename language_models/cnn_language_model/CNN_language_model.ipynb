{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lanugage Modeling Using CNNs and Gated Linear Units\n",
    "\n",
    "Based on the paper: <i>Language Modeling with Gated Convolutional Networks</i>. The sequential nature of the text is modeled by causal convolutions - which use left-zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy,CategoricalCrossentropy\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "words = gutenberg.words('melville-moby_dick.txt')\n",
    "sents = gutenberg.sents('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training word embeddings \n",
    "w2v_model = Word2Vec(sents,size=128,window=5,min_count=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260819, 19317, 5401)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted = Counter(words)\n",
    "common_words = [key for key in counted.keys() if counted[key]>3]\n",
    "len(words),len(set(words)),len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5398\n"
     ]
    }
   ],
   "source": [
    "common_words_final = [] # removing words which don't have embeddings\n",
    "for word in common_words:\n",
    "    if word in w2v_model:\n",
    "        common_words_final.append(word)\n",
    "print(len(common_words_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an encoder and decoder for the words\n",
    "encoder = {}\n",
    "decoder = {}\n",
    "i = 0 # index of an encoding vector\n",
    "for word in common_words_final:\n",
    "    encoder[word] = i\n",
    "    decoder[i] = word\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240233\n"
     ]
    }
   ],
   "source": [
    "train_words = [word for word in words if word in encoder] # used for training, only considering most common words\n",
    "print(len(train_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word_y(word):\n",
    "    \"\"\" encodes word using encoder to one-hot encoding\n",
    "    \"\"\"\n",
    "    word_i = encoder[word]\n",
    "    word_emb = np.zeros((5398))\n",
    "    word_emb[word_i] = 1\n",
    "    return word_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(0,len(train_words)-9,9): # some overlap\n",
    "    temp_x = [w2v_model[word] for word in train_words[i:i+10]]\n",
    "    temp_y = [encode_word_y(word) for word in train_words[i+1:i+10+1]]\n",
    "    x.append(np.array(temp_x))\n",
    "    y.append(np.array(temp_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26692, 10, 128), (26692, 10, 5398))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_convolution_step(x,down,up):\n",
    "    \"\"\" Applies linear convolution step with bottleneck\n",
    "    \"\"\"\n",
    "    in1 = tfa.layers.WeightNormalization(tf.keras.layers.Conv1D(filters=down,kernel_size=1,activation=None))(x)\n",
    "    in2 = tfa.layers.WeightNormalization(tf.keras.layers.Conv1D(filters=down,kernel_size=5,activation=None))(in1)\n",
    "    in3 = tfa.layers.WeightNormalization(tf.keras.layers.Conv1D(filters=up,kernel_size=1,activation=None))(in2)\n",
    "    return in3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x,down,up):\n",
    "    \"\"\" Residual blocks within the network\n",
    "    args:\n",
    "        down: number of filters for downsampling channels\n",
    "        up: number of filters for upsampling channels\n",
    "    \"\"\"\n",
    "    pad = tf.keras.layers.ZeroPadding1D(padding=(4,0))(x)\n",
    "    inter = linear_convolution_step(pad,down,up)\n",
    "    gate = linear_convolution_step(pad,down,up)\n",
    "    gate = tf.math.sigmoid(gate)\n",
    "    out = tf.math.add(tf.multiply(inter,gate),x) # x same shape as output, so no kernel_size=1 convolution\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gated_conv_net():\n",
    "    \"\"\" Network, similar to what was introduced in paper (GCNN-8B), but not as deep/smaller overall\n",
    "        Output of network is the same as the input, & each output is the predicted next word given past context\n",
    "    \"\"\"\n",
    "    x = tf.keras.layers.Input(shape=(10,128))\n",
    "    \n",
    "    # first layer\n",
    "    pad1 = tf.keras.layers.ZeroPadding1D(padding=(4,0))(x)\n",
    "    inter1 = tfa.layers.WeightNormalization(tf.keras.layers.Conv1D(filters=256,kernel_size=5,activation=None))(pad1) # linear convolution\n",
    "    gate1 = tfa.layers.WeightNormalization(tf.keras.layers.Conv1D(filters=256,kernel_size=5,activation=None))(pad1)\n",
    "    gate1 = tf.math.sigmoid(gate1) # applying sigmoid activation to create gate\n",
    "    in1 = tf.keras.layers.Conv1D(filters=256,kernel_size=1)(x) # residual is increased in size before added to out\n",
    "    out1 = tf.math.add(tf.multiply(inter1,gate1),in1) # adding input to output for residual block\n",
    "    \n",
    "    # second layer\n",
    "    out2 = residual_block(out1,down=64,up=256)\n",
    "    out3 = residual_block(out2,down=64,up=256)\n",
    "    \n",
    "    # third layer\n",
    "    out4 = residual_block(out3,down=128,up=256)\n",
    "    out5 = residual_block(out4,down=128,up=256)\n",
    "    \n",
    "    out = tf.keras.layers.Conv1D(filters=5398,kernel_size=1,activation=None)(out5)\n",
    "    model = tf.keras.models.Model(inputs=x,outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gated_conv_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 10, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_2 (ZeroPadding1D (None, 14, 128)      0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_5 (WeightN (None, 10, 256)      328449      zero_padding1d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_4 (WeightN (None, 10, 256)      328449      zero_padding1d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_2 (TensorFl [(None, 10, 256)]    0           weight_normalization_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_2 (TensorFlowOp [(None, 10, 256)]    0           weight_normalization_4[0][0]     \n",
      "                                                                 tf_op_layer_Sigmoid_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 10, 256)      33024       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_2 (TensorFlowOp [(None, 10, 256)]    0           tf_op_layer_Mul_2[0][0]          \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_3 (ZeroPadding1D (None, 14, 256)      0           tf_op_layer_Add_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_9 (WeightN (None, 14, 64)       32961       zero_padding1d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_6 (WeightN (None, 14, 64)       32961       zero_padding1d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_10 (Weight (None, 10, 64)       41153       weight_normalization_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_7 (WeightN (None, 10, 64)       41153       weight_normalization_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_11 (Weight (None, 10, 256)      33537       weight_normalization_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_8 (WeightN (None, 10, 256)      33537       weight_normalization_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_3 (TensorFl [(None, 10, 256)]    0           weight_normalization_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_3 (TensorFlowOp [(None, 10, 256)]    0           weight_normalization_8[0][0]     \n",
      "                                                                 tf_op_layer_Sigmoid_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_3 (TensorFlowOp [(None, 10, 256)]    0           tf_op_layer_Mul_3[0][0]          \n",
      "                                                                 tf_op_layer_Add_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_4 (ZeroPadding1D (None, 14, 256)      0           tf_op_layer_Add_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_15 (Weight (None, 14, 64)       32961       zero_padding1d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_12 (Weight (None, 14, 64)       32961       zero_padding1d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_16 (Weight (None, 10, 64)       41153       weight_normalization_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_13 (Weight (None, 10, 64)       41153       weight_normalization_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_17 (Weight (None, 10, 256)      33537       weight_normalization_16[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_14 (Weight (None, 10, 256)      33537       weight_normalization_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_4 (TensorFl [(None, 10, 256)]    0           weight_normalization_17[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_4 (TensorFlowOp [(None, 10, 256)]    0           weight_normalization_14[0][0]    \n",
      "                                                                 tf_op_layer_Sigmoid_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_4 (TensorFlowOp [(None, 10, 256)]    0           tf_op_layer_Mul_4[0][0]          \n",
      "                                                                 tf_op_layer_Add_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_5 (ZeroPadding1D (None, 14, 256)      0           tf_op_layer_Add_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_21 (Weight (None, 14, 128)      65921       zero_padding1d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_18 (Weight (None, 14, 128)      65921       zero_padding1d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_22 (Weight (None, 10, 128)      164225      weight_normalization_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_19 (Weight (None, 10, 128)      164225      weight_normalization_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_23 (Weight (None, 10, 256)      66305       weight_normalization_22[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_20 (Weight (None, 10, 256)      66305       weight_normalization_19[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_5 (TensorFl [(None, 10, 256)]    0           weight_normalization_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_5 (TensorFlowOp [(None, 10, 256)]    0           weight_normalization_20[0][0]    \n",
      "                                                                 tf_op_layer_Sigmoid_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_5 (TensorFlowOp [(None, 10, 256)]    0           tf_op_layer_Mul_5[0][0]          \n",
      "                                                                 tf_op_layer_Add_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_6 (ZeroPadding1D (None, 14, 256)      0           tf_op_layer_Add_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_27 (Weight (None, 14, 128)      65921       zero_padding1d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_24 (Weight (None, 14, 128)      65921       zero_padding1d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_28 (Weight (None, 10, 128)      164225      weight_normalization_27[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_25 (Weight (None, 10, 128)      164225      weight_normalization_24[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_29 (Weight (None, 10, 256)      66305       weight_normalization_28[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "weight_normalization_26 (Weight (None, 10, 256)      66305       weight_normalization_25[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid_6 (TensorFl [(None, 10, 256)]    0           weight_normalization_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul_6 (TensorFlowOp [(None, 10, 256)]    0           weight_normalization_26[0][0]    \n",
      "                                                                 tf_op_layer_Sigmoid_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Add_6 (TensorFlowOp [(None, 10, 256)]    0           tf_op_layer_Mul_6[0][0]          \n",
      "                                                                 tf_op_layer_Add_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 10, 5398)     1387286     tf_op_layer_Add_6[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 3,693,616\n",
      "Trainable params: 2,558,998\n",
      "Non-trainable params: 1,134,618\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(labels,logits): # reduce mean over batches\n",
    "    return tf.math.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels,logits),axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,_,y_train,_ = train_test_split(x,y,test_size=0.0) # shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.74822533160572\n",
      "84.35508015369291\n",
      "84.25782314356826\n",
      "83.76732313753173\n",
      "82.639131536272\n",
      "80.8979455918617\n",
      "80.3431090709479\n",
      "80.01507155914366\n",
      "80.75396875536576\n",
      "79.31840045479245\n",
      "77.50355033901897\n",
      "77.66948304562254\n",
      "75.4860981252722\n",
      "74.9150687118081\n",
      "74.43922094182699\n",
      "75.37967582727882\n",
      "73.26798600278995\n",
      "72.93294215336195\n",
      "71.9152208255876\n",
      "72.93485900168491\n",
      "70.54453616722377\n",
      "70.63622149383244\n",
      "68.81085899489392\n",
      "69.09649443806408\n",
      "67.74491888391863\n",
      "68.74428725370268\n",
      "67.46187268920768\n",
      "67.6461981520737\n",
      "66.72930756781604\n",
      "65.40196788177057\n",
      "65.71130256600296\n",
      "65.79401166100088\n",
      "64.99836275005312\n",
      "65.81162019821355\n",
      "66.54060789134267\n",
      "65.6869308058516\n",
      "65.67879516636614\n",
      "63.65897870330027\n",
      "63.502684274030656\n",
      "64.86879990595966\n",
      "64.36028458847449\n",
      "63.16489137618575\n",
      "62.77196092908938\n",
      "61.84960123620827\n",
      "64.11596904775539\n",
      "63.51709825899983\n",
      "63.749859873938306\n",
      "62.09140461531026\n",
      "62.74954054480033\n",
      "62.15953446043096\n",
      "62.79326452350074\n",
      "62.55458567970387\n",
      "63.148238394833314\n",
      "61.576232860301076\n",
      "61.61917859642953\n",
      "62.290061675911154\n",
      "62.8345399168286\n",
      "61.37915138194772\n",
      "62.64260584500176\n",
      "62.7400155196529\n",
      "61.897804696815086\n",
      "61.28734561283713\n",
      "61.8179924724899\n",
      "62.79352531262102\n",
      "61.18218811071692\n",
      "61.55821432953994\n",
      "61.60696999500653\n",
      "61.5758645191431\n",
      "59.29289716259612\n",
      "60.45688045457889\n",
      "61.315499233924726\n",
      "61.77912556579949\n",
      "62.05210088926475\n",
      "61.53664220663241\n",
      "61.61720119866364\n",
      "61.050565990262264\n",
      "60.53522445086324\n",
      "60.482354389619985\n",
      "61.64288528164867\n",
      "62.153615971114505\n",
      "61.708243239643714\n",
      "61.75289373365399\n",
      "60.3508919351585\n",
      "60.40716276396879\n",
      "60.329142170657725\n",
      "61.07349007960254\n",
      "60.92062582552461\n",
      "60.91062588997329\n",
      "60.935500235178786\n",
      "61.616643863637925\n",
      "61.49434012470456\n",
      "60.683513870440386\n",
      "62.20961352154313\n",
      "60.094626412938325\n",
      "60.098774246137275\n",
      "59.89477742155812\n",
      "61.582234255203446\n",
      "60.26165922700187\n",
      "59.84011955646407\n",
      "62.55586186131703\n",
      "60.323067740913885\n",
      "60.374876853845926\n",
      "60.62910288680825\n",
      "61.07381397634493\n",
      "61.62685239212087\n",
      "60.78814156874997\n",
      "60.262595205044754\n",
      "61.142952183462874\n",
      "60.915809242711276\n",
      "59.82957191251963\n",
      "61.561731072410346\n",
      "60.66734354939454\n",
      "60.89335367980533\n",
      "58.75981309141087\n",
      "59.27031153367999\n",
      "60.59495821785249\n",
      "60.21009194607411\n",
      "60.640172560773664\n",
      "61.42813250313959\n",
      "60.865864497389694\n",
      "60.12245985924582\n",
      "60.10190706573265\n",
      "59.81608796634708\n",
      "60.539126083044614\n",
      "60.30463463329385\n",
      "60.55611973848364\n",
      "60.753907964537504\n",
      "60.661793629876684\n",
      "60.44159642103412\n",
      "59.56488030108956\n",
      "58.59350045455026\n",
      "60.36241190548749\n",
      "60.701532513714746\n",
      "60.596043381633265\n",
      "59.7008413998777\n",
      "60.43386739640977\n",
      "59.66839065692459\n",
      "60.5411085230011\n",
      "61.026638526123506\n",
      "58.83519359564878\n",
      "61.847815202274276\n",
      "59.38079338455875\n",
      "60.00573888884846\n",
      "60.49296357915665\n",
      "59.077358725673875\n",
      "60.08739643801577\n",
      "59.6397832719434\n",
      "59.44631378392384\n",
      "60.26582455031281\n",
      "59.27276387317908\n",
      "60.01352277034454\n",
      "60.4497718597122\n",
      "59.7440149194639\n",
      "60.37472091643764\n",
      "60.02642116245721\n",
      "60.50153061034302\n",
      "60.35717808789086\n",
      "60.832717719544526\n",
      "60.66755575778243\n",
      "61.213450476445516\n",
      "59.4054980667062\n",
      "60.37414083661158\n",
      "61.12898793254551\n",
      "61.22122479618158\n",
      "60.608450903636474\n",
      "59.60091444209227\n",
      "60.407121649260596\n",
      "61.41759566872886\n",
      "60.14625975573276\n",
      "58.941770375487394\n",
      "59.707347076025\n",
      "60.49364398919734\n",
      "59.53936934604144\n",
      "60.76957609674278\n",
      "59.75061729108399\n",
      "59.886165568421255\n",
      "59.376034293435595\n",
      "58.905216833634604\n",
      "59.23424281680902\n",
      "59.79143717593888\n",
      "60.93316357710859\n",
      "60.54542970176326\n",
      "60.89825859175332\n",
      "59.94853044780561\n",
      "59.53674840691813\n",
      "61.8999110717505\n",
      "60.112158019444685\n",
      "59.832087192148855\n",
      "59.492517824852996\n",
      "60.061607630552196\n",
      "60.44077966248524\n",
      "59.02469519321982\n",
      "60.38763790167979\n",
      "60.29962070164495\n",
      "59.393624352407166\n",
      "59.77513547920016\n",
      "58.8188642876497\n",
      "60.06275383572126\n",
      "60.59720449601801\n",
      "59.17611590165261\n",
      "59.602942435232215\n",
      "58.94610776261665\n",
      "59.869856932993756\n",
      "59.21456429715687\n",
      "59.37026282294501\n",
      "59.73526269920912\n",
      "60.45594316605044\n",
      "59.1263603003404\n",
      "59.71706334954334\n",
      "59.07414255479995\n",
      "60.80926021171447\n",
      "59.41263562558083\n",
      "59.60577742705089\n",
      "59.94270334427347\n",
      "58.69766851070879\n",
      "58.537642626087184\n",
      "60.175985256500816\n",
      "59.347190145128\n",
      "60.210126200670175\n",
      "60.05593474585345\n",
      "59.07360450058375\n",
      "59.200576595784995\n",
      "60.81364136253949\n",
      "59.297385357030926\n",
      "59.018847106473686\n",
      "61.46769592247754\n",
      "58.6814009941218\n",
      "59.520984116434676\n",
      "59.672547639918314\n",
      "59.19478070481754\n",
      "59.77335642934781\n",
      "59.77526372691876\n",
      "60.386848783055264\n",
      "59.24041255796744\n",
      "58.23338167251459\n",
      "59.369314081555075\n",
      "58.72180156423958\n",
      "58.10044571132644\n",
      "60.67886324902328\n",
      "58.769798502472334\n",
      "60.1266765969194\n",
      "59.20218074418417\n",
      "59.811306032381125\n",
      "59.70206834796331\n",
      "60.12338135266939\n",
      "58.65015629037835\n",
      "58.95677092106436\n",
      "60.006992367473195\n",
      "58.583805852243465\n",
      "58.64157307252836\n",
      "58.83053648629275\n",
      "57.77149603511345\n",
      "59.22558332283981\n",
      "59.67071841459493\n",
      "61.093759182326146\n",
      "60.207899183826264\n",
      "59.888403520528854\n",
      "59.27614379108796\n",
      "59.271287216346735\n",
      "59.097696403775124\n",
      "59.197721843467626\n",
      "58.27697979589626\n",
      "58.827122407328524\n",
      "58.501143904974946\n",
      "60.663549063668015\n",
      "58.637392601396314\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1): # training the model\n",
    "    for i in range(0,len(x)-100,100): # batch size of 100\n",
    "        x_subset = x_train[i:i+100]\n",
    "        y_subset = y_train[i:i+100]\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(x_subset,training=True)\n",
    "            loss = cost_function(y_subset,prediction)\n",
    "        print(float(loss))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('margin of the lake -- evinced a wondrous and confidence', (10, 5398))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = y_train[90]\n",
    "words = [decoder[np.argmax(v)] for v in start]\n",
    "words_emb = [w2v_model[word] for word in words]\n",
    "\" \".join(words),start.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100): # simulating 50 word predictions\n",
    "    words_emb_vec = np.expand_dims(np.array(words_emb),axis=0) # (1,10,128)\n",
    "    predicted_words = model(words_emb_vec)\n",
    "    predicted_words = tf.nn.softmax(predicted_words,axis=-1)\n",
    "    pred = int(np.random.choice(list(range(0,5398)),size=1,p=np.squeeze(predicted_words.numpy(),axis=0)[-1]))\n",
    "    #pred = int(np.argmax(np.squeeze(predicted_words.numpy(),axis=0)[-1])) # argmax prediction\n",
    "    pred_word = decoder[pred]\n",
    "    words.append(pred_word)\n",
    "    words_emb.append(w2v_model[pred_word])\n",
    "    words_emb = words_emb[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'margin of the lake -- evinced a wondrous and confidence and hammock . Cabaco again only would for him felt feature of in the vapoury use voices and If like the inhabitants there it he surface were meet with also rather it that voyage descried below ) we him if too , the great measured conduct case been and had . , Your , with this , will him . completely since them As animated , extreme of him scuttle in a pondered from a , in that these the . , till with choice , these In a clinging idea in to then . flames commanded indeed -- after'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
