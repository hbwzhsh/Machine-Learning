{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing neural style transfer model\n",
    "\n",
    "This technique is outlined in the paper - <i>A Neural Algorithm of Artistic Style</i>. Based on the paper, I require a conv net which is trained on a large volume of images and has subsequently learned how to accurately classify images of different classes. I decided to use the VGG-19 model as described in the paper. I am using a [pretrained model](http://www.vlfeat.org/matconvnet/pretrained/#downloading-the-pre-trained-models) and am using a variation of a helper function found on [Github](https://github.com/JudasDie) to quickly read in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import latex\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(10)\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for image height,width\n",
    "n_h = 400\n",
    "n_w = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial images with valid shapes used for testing and first run through model\n",
    "# Creating a randomly generated image\n",
    "\n",
    "gen_img = np.random.rand(n_w,n_h,3)\n",
    "# Loading content and style images\n",
    "content_img = cv2.imread(\"../../data/neural_style/content_1.jpg\") # christmas tree with presents\n",
    "content_img = cv2.resize(content_img, (n_w,n_h), interpolation=cv2.INTER_AREA)\n",
    "style_img = cv2.imread(\"../../data/neural_style/style_1.jpg\") # bright painting\n",
    "style_img = cv2.resize(style_img, (n_w,n_h), interpolation=cv2.INTER_AREA)\n",
    "# Adjusting shapes\n",
    "gen_img.shape = (1,n_w,n_h,3)\n",
    "content_img.shape = (1,n_w,n_h,3)\n",
    "style_img.shape = (1,n_w,n_h,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_1: (1, 300, 400, 64)\n",
      "conv2_1: (1, 150, 200, 128)\n",
      "conv3_1: (1, 75, 100, 256)\n",
      "conv4_1: (1, 38, 50, 512)\n",
      "conv5_1: (1, 19, 25, 512)\n",
      "conv4_2: (1, 38, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "# Getting the output shapes for each layer of the model\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = load_vgg_model(\"../../vgg/imagenet-vgg-verydeep-19.mat\",n_w,n_h,3)\n",
    "    sess.run(model[\"input\"].assign(gen_img))\n",
    "    cv11 = sess.run(model[\"conv1_1\"])\n",
    "    _,cv11w,cv11h,cv11c = cv11.shape\n",
    "    print(\"conv1_1:\",cv11.shape)\n",
    "    cv21 = sess.run(model[\"conv2_1\"])\n",
    "    _,cv21w,cv21h,cv21c = cv21.shape\n",
    "    print(\"conv2_1:\",cv21.shape)\n",
    "    cv31 = sess.run(model[\"conv3_1\"])\n",
    "    _,cv31w,cv31h,cv31c = cv31.shape\n",
    "    print(\"conv3_1:\",cv31.shape)\n",
    "    cv41 = sess.run(model[\"conv4_1\"])\n",
    "    _,cv41w,cv41h,cv41c = cv41.shape\n",
    "    print(\"conv4_1:\",cv41.shape)\n",
    "    cv51 = sess.run(model[\"conv5_1\"])\n",
    "    _,cv51w,cv51h,cv51c = cv51.shape\n",
    "    print(\"conv5_1:\",cv51.shape)\n",
    "    cv42 = sess.run(model[\"conv4_2\"])\n",
    "    print(\"conv4_2:\",cv42.shape)\n",
    "    _,cv42w,cv42h,cv42c = cv42.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building content and style cost functions\n",
    "\n",
    "### Style cost function\n",
    "\n",
    "$$ J_{content}(C,G) = \\frac{1}{2} || a^{[l](C)} - a^{[l](G)} ||^2$$\n",
    "\n",
    "### Content cost function\n",
    "\n",
    "$$ E_l(S,G) = \\frac{1}{4 (n_h n_w)^2 n_c^2} || g^{[l](S)} - g^{[l](G)} ||^2 $$\n",
    "\n",
    "$$ J_{style} = \\sum_{l=0}^{L} w_l * E_l $$\n",
    "\n",
    "### Total cost\n",
    "\n",
    "$$  J_{total} = \\alpha * J_{content} + \\beta * J_{style} $$\n",
    "\n",
    "$$ \\frac{\\alpha}{\\beta} = 1*10^{-3} $$\n",
    "\n",
    "\n",
    "Terms:\n",
    "- C: content image\n",
    "- S: style image\n",
    "- G: generated image\n",
    "- g: gram-matrix between two channels of a given activation\n",
    "- a<sup>(l)(x)</sup>: activation for layer l of image x\n",
    "- w<sub>l</sub>: weighting factors for each layers contribution to the style loss\n",
    "\n",
    "As described in the paper mentioned above, I will be matching the content representation on layer \"conv4_2\". I will be matching the style representation on the layers \"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\" with each receiving an equal weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content cost function\n",
    "def content_cost(cont_act,gen_act):\n",
    "    \"\"\"\n",
    "    cont_act: content image activation for layer \"conv4_2\", shape (1,38,50,512)\n",
    "    gen_act: generated image activation for layer \"conv4_2\"\n",
    "    \"\"\"\n",
    "    m,n_w,n_h,n_c = cont_act.get_shape().as_list()\n",
    "    cont_act_u = tf.reshape(cont_act,(n_w*n_h,n_c)) # shape:(1,n_h*n_w,n_c)\n",
    "    gen_act_u = tf.reshape(gen_act,(n_w*n_h,n_c))\n",
    "    content_diff = tf.square(tf.subtract(cont_act_u,gen_act_u))\n",
    "    content_cost = (1/2) * tf.reduce_sum(content_diff)\n",
    "    return content_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content cost: 141082440000.0\n"
     ]
    }
   ],
   "source": [
    "# Testing content cost function\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = load_vgg_model(\"../../vgg/imagenet-vgg-verydeep-19.mat\",n_w,n_h,3)\n",
    "    # Computation graph\n",
    "    Content = tf.placeholder(tf.float32,(None,cv42w,cv42h,cv42c))\n",
    "    Gen = tf.placeholder(tf.float32,(None,cv42w,cv42h,cv42c))\n",
    "    J_content = content_cost(Content, Gen)\n",
    "    # Getting input data\n",
    "    sess.run(model[\"input\"].assign(content_img)) # getting content image\n",
    "    aContent = sess.run(model[\"conv4_2\"])\n",
    "    sess.run(model[\"input\"].assign(gen_img))\n",
    "    aGen = sess.run(model[\"conv4_2\"])\n",
    "    # Running graph\n",
    "    cost_content = sess.run(J_content,feed_dict={Content:aContent,Gen:aGen})\n",
    "    print(\"Content cost:\",cost_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the gram matrix for a specific matrix of shape (n_c,n_h*n_w)\n",
    "def gram_matrix(act):\n",
    "    act_t = tf.transpose(act)\n",
    "    return tf.matmul(act,act_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes style loss for a single layer\n",
    "def single_layer_style_cost(style_act,gen_act):\n",
    "    m,n_w,n_h,n_c = style_act.get_shape().as_list()\n",
    "    style_act_t = tf.transpose(tf.reshape(style_act, (n_h*n_w, n_c))) # shape: (1,n_c,n_w*n_h)\n",
    "    gen_act_t = tf.transpose(tf.reshape(gen_act, (n_h*n_w, n_c)))\n",
    "    \n",
    "    gram_style = gram_matrix(style_act_t)\n",
    "    gram_gen = gram_matrix(gen_act_t)\n",
    "    style_diff = tf.square(tf.subtract(gram_style,gram_gen))\n",
    "    style_cost = (1/(4 * (n_h*n_w)**2 * n_c**2)) * tf.reduce_sum(style_diff)\n",
    "    return style_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The style cost weights each layers activations equally\n",
    "# Takes a list of activations for each layer corresponding to the generated image and style image (list of tf objs)\n",
    "def style_cost(style_acts,gen_acts):\n",
    "    total_cost = 0\n",
    "    weight = 1 / len(style_acts)\n",
    "    for i in range(len(style_acts)):\n",
    "        temp_cost = single_layer_style_cost(style_acts[i],gen_acts[i])\n",
    "        total_cost += (weight * temp_cost)\n",
    "        \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content cost: 233276720.0\n"
     ]
    }
   ],
   "source": [
    "# Testing the style cost function - with only two distinct layers of activations\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = load_vgg_model(\"../../vgg/imagenet-vgg-verydeep-19.mat\",n_w,n_h,3)\n",
    "    # Computation graph\n",
    "    Gen_1 = tf.placeholder(tf.float32,(None,cv11w,cv11h,cv11c))\n",
    "    Gen_2 = tf.placeholder(tf.float32,(None,cv21w,cv21h,cv21c))\n",
    "    Style_1 = tf.placeholder(tf.float32,(None,cv11w,cv11h,cv11c))\n",
    "    Style_2 = tf.placeholder(tf.float32,(None,cv21w,cv21h,cv21c))\n",
    "    # Getting input data\n",
    "    sess.run(model[\"input\"].assign(gen_img))\n",
    "    agen_1 = sess.run(model[\"conv1_1\"])\n",
    "    agen_2 = sess.run(model[\"conv2_1\"])\n",
    "    sess.run(model[\"input\"].assign(style_img))\n",
    "    astyle_1 = sess.run(model[\"conv1_1\"])\n",
    "    astyle_2 = sess.run(model[\"conv2_1\"])\n",
    "    # Running graph\n",
    "    J_style = style_cost([Style_1,Style_2],[Gen_1,Gen_2])\n",
    "    cost_content = sess.run(J_style,feed_dict={Gen_1:agen_1,Gen_2:agen_2,Style_1:astyle_1,Style_2:astyle_2})\n",
    "    print(\"Content cost:\",cost_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full cost function\n",
    "def cost_function(style_cost,content_cost,alpha=0.0001,beta=1):\n",
    "    return alpha * content_cost + beta * style_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Placeholder initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing placeholders for the generated,style,and content images\n",
    "# takes in a list of sizes(s) which holds the sizes (width,height,channels) for each layer - \n",
    "# Sizes in order of content img sizes first and then style sizes in cronological order\n",
    "def get_placeholders(s):\n",
    "    # Content placeholders\n",
    "    cont = tf.placeholder(tf.float32,shape=(None,s[0],s[1],s[2]))\n",
    "    gen_cont = tf.placeholder(tf.float32,shape=(None,s[0],s[1],s[2]))\n",
    "    # Style placeholders:\n",
    "    # \"conv1_1\"\n",
    "    style_1 = tf.placeholder(tf.float32,shape=(None,s[2],s[4],s[5]))\n",
    "    gen_1 = tf.placeholder(tf.float32,shape=(None,s[2],s[4],s[5]))\n",
    "    # \"conv2_1\"\n",
    "    style_2 = tf.placeholder(tf.float32,shape=(None,s[6],s[7],s[8]))\n",
    "    gen_2 = tf.placeholder(tf.float32,shape=(None,s[6],s[7],s[8]))\n",
    "    # \"conv3_1\"\n",
    "    style_3 = tf.placeholder(tf.float32,shape=(None,s[9],s[10],s[11]))\n",
    "    gen_3 = tf.placeholder(tf.float32,shape=(None,s[9],s[10],s[11]))\n",
    "    # \"conv4_1\"\n",
    "    style_4 = tf.placeholder(tf.float32,shape=(None,s[12],s[13],s[14]))\n",
    "    gen_4 = tf.placeholder(tf.float32,shape=(None,s[12],s[13],s[14]))\n",
    "    # \"conv5_1\"\n",
    "    style_5 = tf.placeholder(tf.float32,shape=(None,s[15],s[16],s[17]))\n",
    "    gen_5 = tf.placeholder(tf.float32,shape=(None,s[15],s[16],s[17]))\n",
    "    \n",
    "    return [cont,gen_cont,style_1,gen_1,style_2,gen_2,style_3,gen_3,style_4,gen_4,style_5,gen_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 38, 50, 512)\n",
      "(?, 38, 50, 512)\n",
      "(?, 512, 400, 64)\n",
      "(?, 512, 400, 64)\n"
     ]
    }
   ],
   "source": [
    "# Testing placeholder function\n",
    "aS = [cv42w,cv42h,cv42c,cv11w,cv11h,cv11c,cv21w,cv21h,cv21c,cv31w,cv31h,cv31c,cv41w,cv41h,cv41c,cv51w,cv51h,cv51c]\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    acts = get_placeholders(aS)\n",
    "    print(acts[0].shape)\n",
    "    print(acts[1].shape)\n",
    "    print(acts[2].shape)\n",
    "    print(acts[3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sizes: list of sizes as described in the get_placeholders function\n",
    "# model: the VGG conv-net\n",
    "# shape: specifies the shape of the images (width,height)\n",
    "def model(gen_img,cont_img,style_img,sizes,shape,lr=0.0001,epochs=10,print_cost=True):\n",
    "    tf.reset_default_graph() # reset graph\n",
    "    costs = []\n",
    "    cont_img.shape = (1,shape[0],shape[1],3) # making sure image shapes are correct\n",
    "    style_img.shape = (1,shape[0],shape[1],3)\n",
    "    gen_img.shape = (1,shape[0],shape[1],3)\n",
    "    \n",
    "    # Computation graph\n",
    "    cont_act,gen_act,style_1,gen_1,style_2,gen_2,style_3,gen_3,style_4,gen_4,style_5,gen_5 = get_placeholders(sizes)\n",
    "    cont_cost = content_cost(cont_act,gen_act)\n",
    "    styl_cost = style_cost([style_1,style_2,style_3,style_4,style_5],[gen_1,gen_2,gen_3,gen_4,gen_5])\n",
    "    cost = cost_function(styl_cost,cont_cost)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        model = load_vgg_model(\"../../vgg/imagenet-vgg-verydeep-19.mat\",shape[0],shape[1],3)\n",
    "        sess.run(init)\n",
    "        # Get the content activations\n",
    "        sess.run(model[\"input\"].assign(content_img))\n",
    "        acont_act = sess.run(model[\"conv4_2\"])\n",
    "        # Get the style activations\n",
    "        sess.run(model[\"input\"].assign(style_img))\n",
    "        astyle_1_act = sess.run(model[\"conv1_1\"])\n",
    "        astyle_2_act = sess.run(model[\"conv2_1\"])\n",
    "        astyle_3_act = sess.run(model[\"conv3_1\"])\n",
    "        astyle_4_act = sess.run(model[\"conv4_1\"])\n",
    "        astyle_5_act = sess.run(model[\"conv5_1\"])\n",
    "        # setting the model for the generated image\n",
    "        sess.run(model[\"input\"].assign(gen_img)) \n",
    "        \n",
    "        for epoch in range(1,epochs+1):\n",
    "            # Getting generated image activations\n",
    "            agen_cont = sess.run(model[\"conv4_2\"])\n",
    "            agen_1_act = sess.run(model[\"conv1_1\"])\n",
    "            agen_2_act = sess.run(model[\"conv2_1\"])\n",
    "            agen_3_act = sess.run(model[\"conv3_1\"])\n",
    "            agen_4_act = sess.run(model[\"conv4_1\"])\n",
    "            agen_5_act = sess.run(model[\"conv5_1\"])\n",
    "            \n",
    "            # Running graph\n",
    "            _,temp_cost = sess.run([optimizer,cost], feed_dict={cont_act:acont_act,gen_act:agen_cont,style_1:astyle_1_act,gen_1:agen_1_act,style_2:astyle_2_act,gen_2:agen_2_act,style_3:astyle_3_act,gen_3:agen_3_act,style_4:astyle_4_act,gen_4:agen_4_act,style_5:astyle_5_act,gen_5:agen_5_act})\n",
    "            costs.append(temp_cost)\n",
    "            \n",
    "            if print_cost == True and epoch % 1 == 0: # always print\n",
    "                print(\"cost at epoch {}: {}\".format(epoch,temp_cost))\n",
    "    \n",
    "        generated_img = sess.run(model['input'])\n",
    "        return generated_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv layer output sizes\n",
    "aSizes = [cv42w,cv42h,cv42c,cv11w,cv11h,cv11c,cv21w,cv21h,cv21c,cv31w,cv31h,cv31c,cv41w,cv41h,cv41c,cv51w,cv51h,cv51c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agen_img = model(gen_img,content_img,style_img,aSizes,[n_w,n_h],lr=0.0001,epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model loader helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the VGG model based on pretrained parameters\n",
    "def load_vgg_model(path,img_height,img_width,num_channels):\n",
    "    vgg = scipy.io.loadmat(path)\n",
    "    vgg_layers = vgg['layers']\n",
    "    \n",
    "    def _weights(layer, expected_layer_name): # Return weights and bias from VGG model for layer\n",
    "        wb = vgg_layers[0][layer][0][0][2]\n",
    "        W = wb[0][0]\n",
    "        b = wb[0][1]\n",
    "        layer_name = vgg_layers[0][layer][0][0][0][0]\n",
    "        assert layer_name == expected_layer_name\n",
    "        return W, b\n",
    "\n",
    "    def _relu(conv2d_layer): # Return relu function\n",
    "        return tf.nn.relu(conv2d_layer)\n",
    "\n",
    "    def _conv2d(prev_layer, layer, layer_name): # Return Conv2d layer using trained weights\n",
    "        W, b = _weights(layer, layer_name)\n",
    "        W = tf.constant(W)\n",
    "        b = tf.constant(np.reshape(b, (b.size)))\n",
    "        return tf.nn.conv2d(prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b\n",
    "\n",
    "    def _conv2d_relu(prev_layer, layer, layer_name): # Returns Conv+Relu using pretrained weights\n",
    "        return _relu(_conv2d(prev_layer, layer, layer_name))\n",
    "\n",
    "    def _avgpool(prev_layer): # Returns AVGPool layer\n",
    "        return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Constructs the graph\n",
    "    graph = {}\n",
    "    graph['input']   = tf.Variable(np.zeros((1, img_height,img_width,num_channels)), dtype = 'float32')\n",
    "    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n",
    "    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n",
    "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
    "    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n",
    "    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n",
    "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
    "    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n",
    "    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n",
    "    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n",
    "    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n",
    "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
    "    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n",
    "    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n",
    "    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n",
    "    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n",
    "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
    "    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n",
    "    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n",
    "    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n",
    "    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n",
    "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a series of images which relate to the holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
