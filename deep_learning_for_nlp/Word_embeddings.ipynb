{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings Using Various Training Strategies\n",
    "\n",
    "Training a skip-gram model on a small amount of data using the following approaches:\n",
    "- standard, as explained in the paper: <i>Efficient Estimation of Word Representations in Vector Space</i>\n",
    "- negative sampling, as explained in the paper: <i>Distributed Representations of Words and Phrases\n",
    "and their Compositionality</i> and highlighted [here](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling)\n",
    "- importance sampling, as explained in the paper: <i>On Using Very Large Target Vocabulary for Neural Machine Translation</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "import nltk\n",
    "import latex\n",
    "from collections import Counter,defaultdict\n",
    "from nltk.corpus import brown\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning news/text data\n",
    "\n",
    "Using the Brown corpus collection of news documents. Only keeping words which have shown up more than two times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of corpus: 3896\n"
     ]
    }
   ],
   "source": [
    "# processing data on word-level, determining which words to keep\n",
    "corpus_words = [word.lower() for word in brown.words(categories='news') if word.isalpha()]\n",
    "word_count = Counter(corpus_words)\n",
    "words_to_keep = set([])\n",
    "for word in corpus_words:\n",
    "    if word_count[word] > 2:\n",
    "        words_to_keep.add(word)\n",
    "\n",
    "print(\"size of corpus:\",len(words_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in corpus: 4498\n"
     ]
    }
   ],
   "source": [
    "# updating the corpus sentences to only keep the valid words\n",
    "corpus_sents = [[word.lower() for word in sent] for sent in brown.sents(categories='news')]\n",
    "corpus_sents = [[word for word in sent if word in words_to_keep] for sent in corpus_sents]\n",
    "corpus_sents = [sent for sent in corpus_sents if len(sent)>1] # keep sents. with atleast two words\n",
    "print(\"Number of sentences in corpus:\",len(corpus_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = [] # getting the new counts for all words\n",
    "for sent in corpus_sents:\n",
    "    for word in sent:\n",
    "        corpus_words.append(word)\n",
    "\n",
    "word_indices = {} # storing unique index per word (for one-hot encoding)\n",
    "word_indices_inv = {} # inverse key-value pairs\n",
    "index = 0\n",
    "for word in words_to_keep:\n",
    "    word_indices_inv[index] = word\n",
    "    word_indices[word] = index\n",
    "    index += 1\n",
    "        \n",
    "word_freq = defaultdict(int) # stores the num. of occurences (frequency) for each word\n",
    "for word in corpus_words:\n",
    "    word_freq[word] += 1\n",
    "    \n",
    "corpus_sents_indices = [[word_indices[word] for word in sent] for sent in corpus_sents] # words->index representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(corpus_sents_indices,window=2):\n",
    "    \"\"\" returns x,y lists which contain context words and associate target word\n",
    "    args:\n",
    "        corpus_sents_indices: sentences in corpus with words represented by one-hot indices\n",
    "        window: the number of words to look at to the left and right of context word\n",
    "    \"\"\"\n",
    "    all_contexts = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for sent in corpus_sents_indices:\n",
    "        sent_contexts = []\n",
    "        sent_targets = []\n",
    "        for i in range(len(sent)):\n",
    "            left_indices = [l for l in range(max(0,i-2),i)]\n",
    "            right_indices = [r for r in range(i+1,min(len(sent),i+2+1))]\n",
    "            left_and_right_indices = left_indices + right_indices\n",
    "            selected_index = random.choice(left_and_right_indices) # selecting one target(index) from the window\n",
    "            target = sent[selected_index]\n",
    "            context = sent[i]\n",
    "            sent_contexts.append(context)\n",
    "            sent_targets.append(target)\n",
    "            \n",
    "        all_contexts += sent_contexts\n",
    "        all_targets += sent_targets\n",
    "        \n",
    "    return all_contexts,all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[508, 3388, 1941, 649, 1517, 558, 3268, 1054, 3141, 1649]\n",
      "[1941, 508, 1517, 3388, 649, 649, 1517, 3268, 1054, 3141]\n"
     ]
    }
   ],
   "source": [
    "all_contexts,all_targets = create_training_pairs(corpus_sents_indices,window=2)\n",
    "print(all_contexts[0:10])\n",
    "print(all_targets[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_encodings(all_contexts,corpus_size=3896):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    vec = np.zeros((len(all_contexts),corpus_size))\n",
    "    for i,context in enumerate(all_contexts):\n",
    "        vec[i][context]=1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74390, 3896)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_contexts_oh = create_one_hot_encodings(all_contexts,corpus_size=3896)\n",
    "all_contexts_oh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97792\n"
     ]
    }
   ],
   "source": [
    "corpus_word_counts = Counter(corpus_words)\n",
    "corpus_word_counts_adj = {} # hold counts used in negative sampling\n",
    "for word in corpus_word_counts:\n",
    "    corpus_word_counts_adj[word] = corpus_word_counts[word]**(3/4) # give greater weight to uncommon words\n",
    "base_sum = sum(list(i[1] for i in corpus_word_counts_adj.items())) # denominator for producing probability\n",
    "\n",
    "sample_vector = []\n",
    "for word in corpus_word_counts_adj:\n",
    "    corpus_word_counts_adj[word] /= base_sum # generating probability of this word's occurance\n",
    "    word_index = word_indices[word]\n",
    "    word_count = int(corpus_word_counts_adj[word]*100000) # number of times to add the index to the sample_vector\n",
    "    sample_vector += [word_index for i in range(word_count)]\n",
    "\n",
    "print(len(sample_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generating data for use in the negative sampling\n",
    "num_negative_samples = 5\n",
    "neg_sampling_indices = []\n",
    "for i in range(len(all_contexts)):\n",
    "    target = all_targets[i]\n",
    "    neg_indices_i = [target] # target is always the first index\n",
    "    num_valid_neg_samples = 0\n",
    "    while num_valid_neg_samples != num_negative_samples:\n",
    "        sample = random.choice(sample_vector) # selecting a random word from the sample_vector\n",
    "        if sample not in neg_indices_i: # preventing the same word to be selected twice or being equal to target\n",
    "            neg_indices_i.append(sample)\n",
    "            num_valid_neg_samples += 1\n",
    "    neg_sampling_indices.append(neg_indices_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74390, 6)\n"
     ]
    }
   ],
   "source": [
    "neg_sampling_indices = np.array(neg_sampling_indices)\n",
    "neg_sampling_indices = neg_sampling_indices.astype(\"int32\")\n",
    "print(neg_sampling_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generating data for use in importance sampling inspired method\n",
    "T = 500 # the number of unique target words per partition (size of sub-vocabulary)\n",
    "all_partition_targets = []\n",
    "all_partition_contexts = []\n",
    "i=0\n",
    "while i < len(all_targets):\n",
    "    these_target_words = set([]) # keeps track of the unique target words in this partition\n",
    "    num_target_words = 0\n",
    "    partition_contexts = []\n",
    "    partition_targets = []\n",
    "    while num_target_words<T and i<len(all_targets):\n",
    "        context,target = all_contexts_oh[i],all_targets[i]\n",
    "        partition_contexts.append(context)\n",
    "        partition_targets.append(target)\n",
    "        i += 1\n",
    "        if target not in these_target_words:\n",
    "            these_target_words.add(target)\n",
    "            num_target_words += 1\n",
    "            \n",
    "    all_partition_targets.append(partition_targets)\n",
    "    all_partition_contexts.append(partition_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "1838\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(all_partition_targets)) # number of partitions\n",
    "print(len(all_partition_targets[0])) # number of examples in partition\n",
    "print(len(set(all_partition_targets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_contexts = []\n",
    "partition_targets = []\n",
    "\n",
    "for partition_i in range(len(all_partition_targets)):\n",
    "    partition_contexts.append(np.array(all_partition_contexts[partition_i]))\n",
    "    these_targets = all_partition_targets[partition_i]\n",
    "    these_unique_targets = list(set(these_targets))\n",
    "    new_unique_targets = [] # the indices information to use for this example\n",
    "    for target in these_targets:\n",
    "        new_unique_target_i = [target]+[t for t in these_unique_targets if t != target]\n",
    "        new_unique_targets.append(new_unique_target_i)\n",
    "        \n",
    "    partition_targets.append(np.array(new_unique_targets).astype(\"int32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1838, 3896)\n",
      "(1838, 500)\n"
     ]
    }
   ],
   "source": [
    "print(partition_contexts[0].shape)\n",
    "print(partition_targets[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec():\n",
    "    \"\"\" Standard model\n",
    "    \"\"\"\n",
    "    x = Input(shape=(3896)) # x represents one-hot encoding, shape:3896x1\n",
    "    h = Dense(300,use_bias=False,activation=None)(x)\n",
    "    o = Dense(3896,use_bias=False,activation=None)(h) # logits, output size of vocabulary\n",
    "    \n",
    "    model = Model(inputs=x,outputs=o)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_loss(labels,logits):\n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_true=labels,y_pred=logits,from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec()\n",
    "optimizer = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.064018602932201\n",
      "6.398518884642785\n",
      "6.1008258811566005\n",
      "5.782794636798506\n",
      "5.466340167101691\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    losses = []\n",
    "    for i in range(0,len(all_contexts_oh)-25,25): # batch size of 25\n",
    "        x_subset = all_contexts_oh[i:i+25]\n",
    "        y_subset = all_targets[i:i+25]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model([x_subset])\n",
    "            loss = standard_loss(y_subset,predictions)\n",
    "        losses.append(float(loss))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(sum(losses)/max(len(losses),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec with Negative Sampling\n",
    "\n",
    "$$ loss: -log[p(w)] - \\sum_{w:w_{neg}} log[1-p(w)] $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_neg():\n",
    "    \"\"\" Model using negative sampling\n",
    "    \"\"\"\n",
    "    x = Input(shape=(3896)) # x represents one-hot encoding, shape:(batch_size,3896)\n",
    "    i = Input(shape=(6),dtype=tf.int32) # 5 indices to extract\n",
    "    \n",
    "    h = Dense(300,use_bias=False,activation=None)(x)\n",
    "    o = Dense(3896,use_bias=False,activation=None)(h) # logits, output size of vocabulary\n",
    "    out = tf.gather(o,i,batch_dims=1) # extracting the relevant dims \n",
    "    \n",
    "    model = Model(inputs=[x,i],outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_loss(logits):\n",
    "    \"\"\" First dimension of logits is the target word, the remaining [1:] are non-target words\n",
    "    \"\"\"\n",
    "    scaled_output = Activation(\"sigmoid\")(logits)\n",
    "    pos_output = scaled_output[:,0]\n",
    "    neg_output = scaled_output[:,1:]\n",
    "    pos_loss_component = tf.reduce_mean(-tf.log(pos_output))\n",
    "    neg_loss_component = tf.reduce_mean(tf.reduce_sum(-tf.log(1-neg_output),axis=-1))\n",
    "    total_loss = pos_loss_component + neg_loss_component\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec_neg()\n",
    "optimizer = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.020754607945931\n",
      "2.4456017694152705\n",
      "2.1599474278618307\n",
      "1.8523982659107496\n",
      "1.5894160255864889\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    losses = []\n",
    "    for i in range(0,len(all_contexts_oh)-25,25): # batch size of 25\n",
    "        x_subset = all_contexts_oh[i:i+25]\n",
    "        # y_subset = all_targets[i:i+25]\n",
    "        i_subset = neg_sampling_indices[i:i+25] # indices\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model([x_subset,i_subset]) # logits\n",
    "            loss = neg_loss(predictions)\n",
    "        losses.append(float(loss))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(sum(losses)/max(len(losses),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec with Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_imp():\n",
    "    \"\"\" Model using importance sampling\n",
    "        Uses the standard_loss function, with y-labels always being the 0-th index\n",
    "    \"\"\"\n",
    "    x = Input(shape=(3896)) # x represents one-hot encoding, shape:(batch_size,3896)\n",
    "    i = Input(shape=(500),dtype=tf.int32) # 500 indices to extract, for reduced softmax\n",
    "    \n",
    "    h = Dense(300,use_bias=False,activation=None)(x)\n",
    "    o = Dense(3896,use_bias=False,activation=None)(h) # logits, output size of vocabulary\n",
    "    out = tf.gather(o,i,batch_dims=1) # extracting the relevant dims \n",
    "    \n",
    "    model = Model(inputs=[x,i],outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_loss(labels,logits):\n",
    "    return tf.reduce_mean(sparse_categorical_crossentropy(y_true=labels,y_pred=logits,from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec_imp()\n",
    "optimizer = Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.746219926947593\n",
      "5.325782788694509\n",
      "4.997750015900375\n",
      "4.644884868841706\n",
      "4.317888594302396\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    losses = []\n",
    "    for partition_i in range(len(partition_targets)): # looping through each partition\n",
    "        targets_i = partition_targets[partition_i] # the indices of vocabulary for this partition\n",
    "        contexts_i = partition_contexts[partition_i]\n",
    "        for i in range(0,len(targets_i)-25,25): # batch size of 25, looping through data per partition\n",
    "            x_subset = contexts_i[i:i+25]\n",
    "            y_subset = np.zeros((25))\n",
    "            i_subset = targets_i[i:i+25] # indices\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model([x_subset,i_subset]) # logits\n",
    "                loss = standard_loss(y_subset,predictions)    \n",
    "            losses.append(float(loss))\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(sum(losses)/max(len(losses),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
