{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the Convolutional Seq2Seq Model for Date Normalization\n",
    "\n",
    "Based on the paper: <i>Convolutional Sequence to Sequence Learning</i>. The particular normalization and initialization strategies highlighted in the paper were not followed directly, along with small details including the size of the hidden representations/number of layers, and my use of separate embedding layers for the input and output text. Task is date normalization/translation at the character level. Position vectors used are described [here](https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model) and [here](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Embedding,Input,Conv1D,ZeroPadding1D,Dense,Dot,Reshape,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.insert(1,'../helpers/')\n",
    "from nmt_utils import *\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 30) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# data for the date normalization task\n",
    "# human_vocab is characters, numbers, and certain symbols\n",
    "# machine_vocab is numbers, and the \"-\" symbol\n",
    "# inv_machine_vocab is translation of model prediction argmax to character\n",
    "# the vocab encoding for the human and machine text is not equivalent\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m=10000)\n",
    "X,Y,Xoh,Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx=30, Ty=10) # output is len 10. assume max input length is 30\n",
    "# vocab size for machine_vocab is 12, and 37 for the human_vocab\n",
    "print(X.shape,Y.shape) # used for the keras embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3] # (human_input, machine_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.,  2., 10., 10.,  9.,  0.,  1.,  6.,  0.,  1.],\n",
       "       [11.,  3.,  1.,  2., 10.,  0.,  2.,  2.,  0.,  2.],\n",
       "       [11.,  2., 10.,  8.,  1.,  0.,  1., 10.,  0.,  2.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in order to prevent future information contaminating the model's predictions, the start of the input to the decoder must have a new symbol\n",
    "# the new symbol will essentially represent zero-padding/no-information, and will be added to the machine vocab\n",
    "# note: this does not follow the paper directly, beccause I know the output of this problem is a FIXED size\n",
    "machine_vocab['<s>'] = 11 # delimiter for predictions\n",
    "inv_machine_vocab[11] = '<s>'\n",
    "# additionally, the right-most element will be deleted, given that the model will never know the final prediction (last element)\n",
    "decoder_start = np.ones((10000,1))*11\n",
    "decoder_input = np.hstack([decoder_start,Y[:,:9]])\n",
    "decoder_input[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring the datatypes are correct\n",
    "decoder_input = decoder_input.astype('float64')\n",
    "X = X.astype('float64')\n",
    "Y = Y.astype('float64')\n",
    "Yoh = Yoh.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., 10., 10.,  9.,  0.,  1.,  6.,  0.,  1., 10.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0] # y-label for the first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yoh[0] # one-hot encoding y-label for the first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_vectors(num_positions):\n",
    "    \"\"\" returns position vectors of shape num_positionsx128\n",
    "    args:\n",
    "        num_positions: length of the input\n",
    "    \"\"\"\n",
    "    position_embeddings = []\n",
    "    positions = [i for i in range(num_positions)]\n",
    "    d=128 # the vector size\n",
    "    \n",
    "    for pos in positions: # creating an embedding for each item in sequence\n",
    "        emb = []\n",
    "        for i in range(0,64):\n",
    "            emb.append(math.sin(pos/(10000**(2*i/d))))\n",
    "            emb.append(math.cos(pos/(10000**(2*i/d))))\n",
    "        emb = np.array(emb)\n",
    "        position_embeddings.append(emb)\n",
    "    \n",
    "    position_embeddings = np.array(position_embeddings)\n",
    "    position_embeddings = position_embeddings.astype('float64')\n",
    "    return position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings = get_position_vectors(10)\n",
    "position_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x,k,l_pad,r_pad):\n",
    "    \"\"\" Residual GLU block for both the encoder and decoder - dimensionality is fixed at 128\n",
    "    args:\n",
    "        l_pad: amount of left padding\n",
    "        r_pad: amount of right padding\n",
    "    \"\"\"\n",
    "    pad = ZeroPadding1D(padding=(l_pad,r_pad))(x)\n",
    "    A = Conv1D(filters=128,kernel_size=k,padding='valid')(pad)\n",
    "    B = Conv1D(filters=128,kernel_size=k,padding='valid')(pad)\n",
    "    gate = tf.math.sigmoid(B)\n",
    "    out = tf.multiply(A,gate)\n",
    "    out = tf.math.add(out,x) # adding back input\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x,num_blocks=6):\n",
    "    \"\"\" encoder portion of the model\n",
    "    \"\"\"\n",
    "    for _ in range(num_blocks):\n",
    "        x = residual_block(x,k=3,l_pad=1,r_pad=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(x,decoder_emb,encoding,input_emb):\n",
    "    \"\"\" takes in the input to the decoder block and computes output representation\n",
    "    args:\n",
    "        x: input to this decoder_block\n",
    "        encoding: output from the encoder (z_j in the paper)\n",
    "        input_emb: input to the encoder (e_j in the paper)\n",
    "        decoder_emb: input to the decoder (g_i in the paper)\n",
    "    \"\"\"\n",
    "    h = residual_block(x,k=5,l_pad=4,r_pad=0)\n",
    "    d = Dense(128)(h)+decoder_emb\n",
    "    pre_att_num = K.exp(Dot(axes=-1)([d,encoding])) # numerator for att calculation\n",
    "    pre_att_denom = K.repeat_elements(K.expand_dims(K.sum(pre_att_num,axis=-1),axis=-1),rep=30,axis=-1) # denominator for att calc, repeated over last dim.\n",
    "    att = tf.divide(pre_att_num,pre_att_denom) # element-wise division, scaled attention, shape:Nx10x30\n",
    "    x = encoding+input_emb # vectors to multiply with attention values\n",
    "    \n",
    "    c = tf.matmul(att,x)\n",
    "    c = c+h # adding back in the output of the GLU decoder block, before attention\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(encoding,decoder_emb,input_emb):\n",
    "    \"\"\" decoder portion of the model\n",
    "    args:\n",
    "        encoding: output from the encoder\n",
    "        input_emb: input to the encoder\n",
    "        decoder_emb: input to the decoder\n",
    "    \"\"\"\n",
    "    c1 = decoder_block(decoder_emb,decoder_emb,encoding,input_emb)\n",
    "    c2 = decoder_block(c1,decoder_emb,encoding,input_emb)\n",
    "    c3 = decoder_block(c2,decoder_emb,encoding,input_emb) # I found that only using two decoder layers didn't work\n",
    "    pred = Dense(11)(c3) # prediction output does not include the '<s>' symbol, b/c it will never show up in y-labels\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_seq2seq_model():\n",
    "    \"\"\" Conv seq2seq model implementation\n",
    "    \"\"\"\n",
    "    x = Input(shape=(30)) # input to the encoder\n",
    "    y = Input(shape=(10)) # input to decoder\n",
    "    position_emb = Input((30,128))\n",
    "    \n",
    "    input_emb = Embedding(37,128)(x)\n",
    "    input_emb = input_emb+position_emb\n",
    "    encoding = encoder(input_emb)\n",
    "    \n",
    "    decoder_emb = Embedding(12,128)(y) # includes an embedding for '<s>'\n",
    "    decoder_emb = decoder_emb+position_emb[:,:10,:]\n",
    "    decoder_prediction = decoder(encoding,decoder_emb,input_emb)\n",
    "    \n",
    "    model = Model(inputs=[x,y,position_emb],outputs=decoder_prediction)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_seq2seq_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(labels,logits): # reduce mean over batches\n",
    "    return tf.math.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels,logits=logits),axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.9059443121059\n",
      "6.265195037635749\n",
      "4.32493228305248\n"
     ]
    }
   ],
   "source": [
    "# training the model, loss values continued from previous training\n",
    "position_emb = get_position_vectors(30)\n",
    "position_embedding = np.array([position_emb for i in range(100)])\n",
    "\n",
    "for _ in range(3):\n",
    "    losses = []\n",
    "    for i in range(0,len(X)-100,100): # batch size of 100\n",
    "        x_subset = X[i:i+100] # input to encoder\n",
    "        y_subset = Yoh[i:i+100] # one-hot labels\n",
    "        y_delayed = decoder_input[i:i+100] # input to decoder\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction_logits = model([x_subset,y_delayed,position_embedding])\n",
    "            loss = cost_function(y_subset,prediction_logits)\n",
    "            \n",
    "        losses.append(float(loss))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_subset = np.expand_dims(X[11],axis=0)\n",
    "#y_subset = np.expand_dims(Yoh[3],axis=0)\n",
    "y_delayed = np.expand_dims(decoder_input[11],axis=0)\n",
    "position_emb = np.expand_dims(get_position_vectors(30),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1978-06-16'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = K.softmax(model([x_subset,y_delayed,position_emb])).numpy()[0]\n",
    "\"\".join([inv_machine_vocab[num] for num in list(np.argmax(pred,axis=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('friday june 16 1978', '1978-06-16')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[11]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
