{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of ByteNet for Character Level Translation\n",
    "\n",
    "Based on the paper: <i>Neural Machine Translation in Linear Time</i>. Task is date normalization at the character level. Here, the choice was made to have the output of the encoder be the same size as the input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.insert(1,'../helpers/')\n",
    "from nmt_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 30, 37) (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "# data for the date normalization task\n",
    "# human_vocab is characters, numbers, and certain symbols\n",
    "# machine_vocab is numbers, and the \"-\" symbol\n",
    "# inv_machine_vocab is translation of model prediction argmax to character\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m=10000)\n",
    "X,Y,Xoh,Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx=30, Ty=10) # output is len 10. assume max input length is 30\n",
    "print(Xoh.shape,Yoh.shape) # one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3] # (human_input, machine_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delayed_output = [] # second input to model, simulates dynamic unfolding with each sequential prediction\n",
    "for ex in Yoh:\n",
    "    temp_delayed = np.zeros((10,11))\n",
    "    temp_delayed[1:,:] = ex[:9,:] # first input will be a vector of zeros\n",
    "    delayed_output.append(temp_delayed)\n",
    "\n",
    "delayed_output = np.array(delayed_output)\n",
    "delayed_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delayed_output[10] # first row is zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x,d,dilation,decoder):\n",
    "    # input processing\n",
    "    norm1 = tf.keras.layers.LayerNormalization()(x) # 2*d=256 channels\n",
    "    relu1 = tf.nn.relu(norm1)\n",
    "    # first convolution\n",
    "    conv1 = tf.keras.layers.Conv1D(filters=d,kernel_size=1,activation=None)(relu1) # decrease channel size\n",
    "    conv1 = tf.keras.layers.LayerNormalization()(conv1)\n",
    "    relu2 = tf.nn.relu(conv1)\n",
    "    # dilated convolution\n",
    "    if decoder: # decoder block requires masked convolutions\n",
    "        paddings = tf.constant([[0,0],[2*dilation,0],[0,0]]) # prevents being able to see future tokens\n",
    "        relu2 = tf.pad(relu2,paddings)\n",
    "        conv2 = tf.keras.layers.Conv1D(filters=d,kernel_size=3,activation=None,dilation_rate=dilation,padding='valid')(relu2)\n",
    "    else:\n",
    "        conv2 = tf.keras.layers.Conv1D(filters=d,kernel_size=3,activation=None,dilation_rate=dilation,padding='same')(relu2)\n",
    "    conv2 = tf.keras.layers.LayerNormalization()(conv2)\n",
    "    relu3 = tf.nn.relu(conv2)\n",
    "    # last convolution, including adding back input to residual block\n",
    "    conv3 = tf.keras.layers.Conv1D(filters=2*d,kernel_size=1,activation=None)(relu3) # increase channel size\n",
    "    out = tf.math.add(conv3,x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byte_net(d=128,dilations=[1,2,4,8]):\n",
    "    \"\"\" ByteNet implementation, with decreased dimensionality\n",
    "    args:\n",
    "        dilations: [1,2,4,8]; d=128 (2d=256); filter_width=3\n",
    "        x: input sequence\n",
    "        delayed_output: output sequence but off by one, with the first value being all zeros (this allows the model to incorporate prior model predictions into future predictions)\n",
    "    \"\"\"\n",
    "    x = tf.keras.layers.Input(shape=(30,37))\n",
    "    delayed_output = tf.keras.layers.Input(shape=(10,11))\n",
    "    \n",
    "    input_emb = tf.keras.layers.Conv1D(filters=2*d,kernel_size=1,activation=None)(x) # get input embeddings\n",
    "    for _ in range(2):\n",
    "        for dilation in dilations:\n",
    "            input_emb = residual_block(input_emb,d,dilation,decoder=False)\n",
    "    \n",
    "    # if the translation can be longer than the input embedding, then need to right-pad the input embedding up to the length of output translation (not relevant for this problem)\n",
    "    input_emb = input_emb[:,0:delayed_output.shape[1],:] # matching the size of the decoder input, means that at test time it can take in the predictions one at a time\n",
    "    output_emb = tf.keras.layers.Conv1D(filters=2*d,kernel_size=1,activation=None)(delayed_output)\n",
    "\n",
    "    decoder_emb = tf.math.add(input_emb,output_emb) # adding input embedding to the delayed output embedding (incorporates data from embedding of input sequence and \"previous\" predictions)\n",
    "    for _ in range(2):\n",
    "        for dilation in dilations:\n",
    "            decoder_emb = residual_block(decoder_emb,d,dilation,decoder=True)\n",
    "    \n",
    "    out_layer_norm = tf.keras.layers.LayerNormalization()(decoder_emb)\n",
    "    out_relu = tf.nn.relu(out_layer_norm)\n",
    "    out_conv = tf.keras.layers.Conv1D(filters=11,kernel_size=1,activation=None)(out_relu) # logits, channels = output_vocab_size = 11\n",
    "    \n",
    "    model = Model(inputs=[x,delayed_output],outputs=out_conv)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = byte_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(labels,logits): # reduce mean over batches\n",
    "    return tf.math.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels,logits),axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.224599323785586\n",
      "21.41615614345207\n",
      "20.065162571311397\n",
      "19.719816056183898\n",
      "19.084941050007473\n",
      "18.488159875325934\n",
      "18.49901135883115\n",
      "18.093302831972906\n",
      "17.55586824810872\n",
      "17.146835231063793\n",
      "16.784526613533732\n",
      "16.86618459532378\n",
      "16.698407022320243\n",
      "16.28484881025251\n",
      "16.012727609514677\n",
      "15.680726292135406\n",
      "15.659668630066049\n",
      "15.229047245247283\n",
      "14.752595980825765\n",
      "14.592712405631564\n",
      "14.63669792980527\n",
      "14.41851311493644\n",
      "14.092955882878769\n",
      "14.09978065970398\n",
      "13.679736100531736\n",
      "13.445947321415343\n",
      "13.329682993281972\n",
      "13.251341580551047\n",
      "12.848640296532299\n",
      "13.059524296053674\n",
      "12.812581193864379\n",
      "12.731786249607701\n",
      "12.620502044598053\n",
      "12.130838511139755\n",
      "11.945021316899311\n",
      "12.0219872550002\n",
      "11.846380263065976\n",
      "11.833849009712116\n",
      "11.813696271334388\n",
      "11.666687953944134\n",
      "11.38517083259419\n",
      "11.414754889411453\n",
      "11.12972357803784\n",
      "11.29239691354657\n",
      "11.189892560951897\n",
      "11.182275488593817\n",
      "11.07371209367794\n",
      "11.164510313623063\n",
      "10.836488259560515\n",
      "11.044693639032321\n",
      "10.785222167731456\n",
      "11.01023278850737\n",
      "10.809956582029697\n",
      "10.664164835710778\n",
      "10.512644578822941\n",
      "10.350998694262676\n",
      "10.70446553358285\n",
      "10.470803448305581\n",
      "10.332980712520241\n",
      "10.228008122298723\n",
      "10.177006958446727\n",
      "10.084907199256383\n",
      "10.082139939958587\n",
      "10.239383385447411\n",
      "9.976699564137187\n",
      "9.810115017696795\n",
      "9.953259331855987\n",
      "9.74017821825379\n",
      "9.898830293507991\n",
      "9.790299529564743\n",
      "10.038251938441624\n",
      "9.924469204731116\n",
      "9.879049207028846\n",
      "9.540530945965465\n",
      "9.678416174042844\n",
      "9.488304567921285\n",
      "9.401697710737135\n",
      "9.294586527001757\n",
      "9.26200671597042\n",
      "9.263853312343649\n",
      "9.113965829693564\n",
      "9.252774631374123\n",
      "9.438160258554687\n",
      "9.399393438361622\n",
      "8.864818232847142\n",
      "8.69263005347304\n",
      "9.043835372074124\n",
      "8.989044099872974\n",
      "8.806631715073394\n",
      "8.948884447215606\n",
      "8.759505495227447\n",
      "8.69798778806733\n",
      "8.653219815016334\n",
      "8.58009705294293\n",
      "8.5909930471587\n",
      "8.666025867092667\n",
      "8.29062244120942\n",
      "8.608265463594451\n",
      "8.706876880718509\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1): # training the model\n",
    "    for i in range(0,len(Xoh)-100,100): # batch size of 100\n",
    "        x_subset = Xoh[i:i+100]\n",
    "        y_subset = Yoh[i:i+100]\n",
    "        delayed_output_subset = delayed_output[i:i+100]\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model([x_subset,delayed_output_subset],training=True)\n",
    "            loss = cost_function(y_subset,prediction)\n",
    "        print(float(loss))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('9/10/70', '1970-09-10')\n",
      "(1, 30, 37) (1, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "# prediction example from training set\n",
    "example_i = 2\n",
    "adelayed_output = np.zeros((1,10,11)) # initialized to zeros, prior predictions added one at a time\n",
    "ainput = np.expand_dims(Xoh[example_i],axis=0)\n",
    "ex = dataset[example_i]\n",
    "print(ex)\n",
    "print(ainput.shape,adelayed_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1975-09-19\n"
     ]
    }
   ],
   "source": [
    "# from one epoch of training\n",
    "pred_string = ''\n",
    "for i in range(0,10): # output is guaranteed to be 10 symbols\n",
    "    out_i = model([ainput,adelayed_output])\n",
    "    out_i = tf.nn.softmax(out_i[0,i,:],axis=-1)\n",
    "    char_i = np.argmax(out_i)\n",
    "    char_pred = inv_machine_vocab[char_i]\n",
    "    pred_string += str(char_pred)\n",
    "    out_i = np.zeros((11))\n",
    "    out_i[char_i] = 1\n",
    "    if i != 9:\n",
    "        adelayed_output[0,i+1,:] = out_i\n",
    "\n",
    "print(pred_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
