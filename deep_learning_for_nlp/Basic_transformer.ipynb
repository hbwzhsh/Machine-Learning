{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Implementation\n",
    "\n",
    "Based on the following paper: <i>Attention Is All You Need</i>. The following are good guides: [here](http://www.peterbloem.nl/blog/transformers) and [here](http://jalammar.github.io/illustrated-transformer/). Task is date normalization/translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Embedding,Input,Dense,Dot,LayerNormalization,Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import numpy as np\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.insert(1,'../helpers/')\n",
    "from nmt_utils import *\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for the date normalization task\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 30) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# adjusting the data so that the vocab is shared between input and output\n",
    "## this means that the X and Y data (input to encoder/decoder) use the human_vocab dictionary,\n",
    "## but the output of the decoder will still use the machine_vocab dictionary\n",
    "human_vocab['-'] = 37\n",
    "human_vocab['<s>'] = 38\n",
    "X,Y,_,_ = preprocess_data(dataset, human_vocab=human_vocab, machine_vocab=human_vocab, Tx=30, Ty=10) # used as input to the encoder/decoder\n",
    "_,_,Xoh,Yoh = preprocess_data(dataset, human_vocab=human_vocab, machine_vocab=machine_vocab, Tx=30, Ty=10) # used as labels for output of the decoder\n",
    "print(X.shape,Y.shape) # used for the keras embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'), ('10.11.19', '2019-11-10')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:2] # (human_input, machine_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 12, 12, 11, 37,  3,  8, 37,  3, 12],\n",
       "       [ 5,  3,  4, 12, 37,  4,  4, 37,  4,  3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38.,  4., 12., 12., 11., 37.,  3.,  8., 37.,  3.],\n",
       "       [38.,  5.,  3.,  4., 12., 37.,  4.,  4., 37.,  4.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in order to prevent future information contaminating the model's predictions, the start of the input to the decoder must have a new symbol\n",
    "# the new symbol will essentially represent zero-padding/no-information\n",
    "# additionally, the right-most element will be deleted, given that the model will never know the final prediction (last element)\n",
    "decoder_start = np.ones((10000,1))*38 # the symbol '<s>'\n",
    "decoder_input = np.hstack([decoder_start,Y[:,:9]])\n",
    "decoder_input[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring the datatypes are correct\n",
    "decoder_input = decoder_input.astype('float64') # input to decoder\n",
    "X = X.astype('float64') # input to encoder\n",
    "Yoh = Yoh.astype('float64') # labels for output of decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_vectors(num_positions):\n",
    "    \"\"\" returns position vectors of shape num_positionsx256\n",
    "    args:\n",
    "        num_positions: length of the input\n",
    "    \"\"\"\n",
    "    position_embeddings = []\n",
    "    positions = [i for i in range(num_positions)]\n",
    "    d=256 # the vector size\n",
    "    \n",
    "    for pos in positions: # creating an embedding for each item in sequence\n",
    "        emb = []\n",
    "        for i in range(0,128):\n",
    "            emb.append(math.sin(pos/(10000**(2*i/d))))\n",
    "            emb.append(math.cos(pos/(10000**(2*i/d))))\n",
    "        emb = np.array(emb)\n",
    "        position_embeddings.append(emb)\n",
    "    \n",
    "    position_embeddings = np.array(position_embeddings)\n",
    "    position_embeddings = position_embeddings.astype('float64')\n",
    "    return position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings = get_position_vectors(10)\n",
    "position_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset_mask(seq_len=10):\n",
    "    \"\"\" Returns the mask for the decoder attention mechanism; mask intended to be added pre-softmax\n",
    "    args:\n",
    "        seq_len: length of the input to the decoder\n",
    "    \"\"\"\n",
    "    mask = np.zeros((seq_len,seq_len))\n",
    "    mask_locations = np.triu_indices(seq_len,k=1)\n",
    "    mask[mask_locations] = float(\"-inf\")\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf],\n",
       "       [  0.,   0., -inf],\n",
       "       [  0.,   0.,   0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = get_offset_mask(seq_len=3)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_layer(Q_x,KV_x,mask=None):\n",
    "    \"\"\" Individual attention block for the encoder/decoder (dim=64)\n",
    "    args:\n",
    "        Q_x: input to caclulate the Q matrix (differs from KV_x in encoder-decoder attention block)\n",
    "        KV_x: input to calculate the K and V matrices\n",
    "        mask: masking for the decoder attention block\n",
    "    \"\"\"\n",
    "    # Dense layers w/ no bias&activation are equivalent to linear transformations:\n",
    "    Q = Dense(64,use_bias=False,activation=None)(Q_x) # queries\n",
    "    K = Dense(64,use_bias=False,activation=None)(KV_x) # keys\n",
    "    V = Dense(64,use_bias=False,activation=None)(KV_x) # values\n",
    "    \n",
    "    unscaled_att_weights = Dot(axes=-1)([Q,K])/tf.cast(tf.sqrt(64.0),tf.float64)\n",
    "    if mask is not None: # only for the decoder layer\n",
    "        unscaled_att_weights += mask\n",
    "    \n",
    "    att_weights = tf.nn.softmax(unscaled_att_weights,axis=-1)\n",
    "    att_output = tf.matmul(att_weights,V)\n",
    "    return att_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(x,h=4):\n",
    "    \"\"\" Encoder block; num_attention_heads=4\n",
    "    args:\n",
    "        h: number of attention heads\n",
    "    \"\"\"\n",
    "    # multi-head attention:\n",
    "    attention_heads=[]\n",
    "    for _ in range(h):\n",
    "        att_output = attention_layer(x,x,mask=None)\n",
    "        attention_heads.append(att_output)\n",
    "    \n",
    "    multi_head_att_output = Concatenate()(attention_heads)\n",
    "    multi_head_att_output = Dense(256,use_bias=False,activation=None)(multi_head_att_output)\n",
    "    attention_output = LayerNormalization()(multi_head_att_output+x) # residual block 1\n",
    "    \n",
    "    # feed-forward:\n",
    "    ffn = Dense(512,activation='relu')(attention_output)\n",
    "    ffn = Dense(256,activation=None)(ffn)\n",
    "    encoder_output = LayerNormalization()(attention_output+ffn) # residual block 2\n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(x,encoder_output,mask,h=4):\n",
    "    \"\"\" Decoder block; num_attention_heads=4\n",
    "    args:\n",
    "        encoder_output: output sequence from encoder\n",
    "    \"\"\"\n",
    "    # decoder multi-head attention:\n",
    "    attention_heads=[]\n",
    "    for _ in range(h):\n",
    "        att_output = attention_layer(x,x,mask=mask)\n",
    "        attention_heads.append(att_output)\n",
    "        \n",
    "    multi_head_att_output = Concatenate()(attention_heads)\n",
    "    multi_head_att_output = Dense(256,use_bias=False,activation=None)(multi_head_att_output)\n",
    "    attention_output_1 = LayerNormalization()(multi_head_att_output+x) # residual block 1\n",
    "    \n",
    "    # encoder-decoder multi-head attention:\n",
    "    attention_heads=[]\n",
    "    for _ in range(h):\n",
    "        att_output = attention_layer(attention_output_1,encoder_output,mask=None)\n",
    "        attention_heads.append(att_output)\n",
    "        \n",
    "    multi_head_att_output = Concatenate()(attention_heads)\n",
    "    multi_head_att_output = Dense(256,use_bias=False,activation=None)(multi_head_att_output)\n",
    "    attention_output_2 = LayerNormalization()(multi_head_att_output+attention_output_1) # residual block 2\n",
    "    \n",
    "    # feed-forward:\n",
    "    ffn = Dense(512,activation='relu')(attention_output_2)\n",
    "    ffn = Dense(256,activation=None)(ffn)\n",
    "    decoder_output = LayerNormalization()(attention_output_2+ffn) # residual block 3\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(embedder):\n",
    "    \"\"\" Transformer implementation: d_model=256; n_encoder_layers=2; n_decoder_layers=2\n",
    "    \"\"\"\n",
    "    x = Input(shape=(30)) # input to the encoder\n",
    "    delayed_y = Input(shape=(10)) # input to decoder\n",
    "    mask = Input(shape=(10,10)) # for the decoder attention calculation\n",
    "    position_emb = Input(shape=(30,256))\n",
    "    \n",
    "    # encoder block\n",
    "    input_emb = embedder(x)\n",
    "    embedding = input_emb+position_emb\n",
    "    e1 = encoder_block(embedding)\n",
    "    e2 = encoder_block(e1)\n",
    "    e3 = encoder_block(e2)\n",
    "    e4 = encoder_block(e3)\n",
    "    \n",
    "    # decoder block\n",
    "    delayed_emb = embedder(delayed_y)\n",
    "    embedding = delayed_emb+position_emb[:,:10,:]\n",
    "    d1 = decoder_block(embedding,e4,mask)\n",
    "    d2 = decoder_block(d1,e4,mask)\n",
    "    d3 = decoder_block(d2,e4,mask)\n",
    "    d4 = decoder_block(d3,e4,mask)\n",
    "    \n",
    "    # model predictions\n",
    "    out = Dense(11,activation=None)(d4) # to output vocab_size=11\n",
    "    \n",
    "    model = Model(inputs=[x,delayed_y,mask,position_emb],outputs=out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedding(39,256)\n",
    "model = transformer(embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(labels,logits): # reduce mean over batches\n",
    "    return tf.math.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels,logits=logits),axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.533532973148834\n",
      "12.040308459114357\n",
      "10.143167755837124\n",
      "10.05425456858201\n",
      "9.9752173483436\n",
      "9.816161877660827\n",
      "9.21421149737957\n",
      "8.461537647061093\n",
      "7.6649314410351925\n",
      "6.934595526335201\n",
      "6.097987069469845\n",
      "5.327165545203649\n",
      "4.391387596383244\n",
      "3.4292546149248784\n",
      "2.485484851235204\n",
      "1.8928204090485177\n",
      "1.3170541442834323\n",
      "0.9330483456528719\n",
      "1.0586879242693292\n",
      "0.6299525997656727\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "position_emb = get_position_vectors(30)\n",
    "position_embedding = np.array([position_emb for i in range(100)])\n",
    "mask = get_offset_mask(seq_len=10)\n",
    "mask = np.array([mask for i in range(100)])\n",
    "\n",
    "for _ in range(20):\n",
    "    losses = []\n",
    "    for i in range(0,len(X)-100,100): # batch size of 100\n",
    "        x_subset = X[i:i+100] # input to encoder\n",
    "        y_subset = Yoh[i:i+100] # one-hot labels\n",
    "        y_delayed = decoder_input[i:i+100] # input to decoder\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction_logits = model([x_subset,y_delayed,mask,position_embedding])\n",
    "            loss = cost_function(y_subset,prediction_logits)\n",
    "            \n",
    "        losses.append(float(loss))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    print(sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 11\n",
    "x_subset = np.expand_dims(X[i],axis=0)\n",
    "y_delayed = np.expand_dims(decoder_input[i],axis=0)\n",
    "position_emb = np.expand_dims(get_position_vectors(30),axis=0)\n",
    "mask = np.expand_dims(get_offset_mask(seq_len=10),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1978-06-16'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = K.softmax(model([x_subset,y_delayed,mask,position_emb])).numpy()[0]\n",
    "\"\".join([inv_machine_vocab[num] for num in list(np.argmax(pred,axis=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('friday june 16 1978', '1978-06-16')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
