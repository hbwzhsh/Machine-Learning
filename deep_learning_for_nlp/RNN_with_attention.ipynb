{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing RNN with Attention for the task of date normalization\n",
    "\n",
    "The technique used is outlined in the paper - <i>Neural Machine Translation by Jointly Learning to Align and Translate</i>. This specific project is a component of the deeplearning.ai deep learning course, and the helper functions used to create the fake date data was ported from their tools (as well as much of the architecture). Here, the model is tasked to translate dates such as \"25th of october 1990\" to \"1990-10-25\". The output of this model is of fixed length, so there is no necessity for there to be a token indicating the end of the encoding. In addition, I built out a standard encoder-decoder network in order to compare performances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import RepeatVector,LSTM,Bidirectional,Dense,ReLU,Softmax\n",
    "from tensorflow.keras.layers import Input,Concatenate,Dot,Activation,Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.insert(1,'../helpers/')\n",
    "from nmt_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 30, 37) (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "# human_vocab is characters, numbers, and certain symbols\n",
    "# machine_vocab is numbers, and the \"-\" symbol\n",
    "# inv_machine_vocab is translation of model prediction argmax to character\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m=10000)\n",
    "X,Y,Xoh,Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx=30, Ty=10) # output is len 10. assume max input length is 30\n",
    "print(Xoh.shape,Yoh.shape) # one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3] # (human_input, machine_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run keras functions with numpy data\n",
    "# sess = tf.InteractiveSession()\n",
    "# a = np.array([1.0,2,3,4,5])\n",
    "# out = softmax(a)\n",
    "# out.eval()\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN With Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized as global layers so their paramters are not re-initialized in model inner-loop\n",
    "repeat = RepeatVector(30) # the max-size of the input, function to be applied\n",
    "concatenate = Concatenate()\n",
    "dense_layer = Dense(1,activation = \"relu\") # single layered DNN, for calculating attention weights\n",
    "softmax = Softmax(axis=1) # is not axis=1, then softmax won't work (b/c shape is (None,30,1))\n",
    "dot_product = Dot(axes=1)\n",
    "post_activation_lstm_cell = LSTM(128,activation=\"tanh\",return_state=True) # returns last state AND output\n",
    "output_layer = Dense(11,activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_step_attention(a,s):\n",
    "    \"\"\" Calculates the context for this timestep\n",
    "    args:\n",
    "        a: output from bidirectional rnn\n",
    "        s: rnn state at the previous timestep; (,128)\n",
    "    \"\"\"\n",
    "    s = repeat(s) # (30,128)\n",
    "    concat = concatenate([a,s]) # (30,256)\n",
    "    e = dense_layer(concat) # (30,1), calculating the energies which represent the unbounded attention weights\n",
    "    att = softmax(e) # (30,1), scaling the attention weights\n",
    "    context = dot_product([att,a]) # (1,128), calculating the input context vector for post-attention lstm\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_len=30,output_len=10,in_vocab_size=37,out_vocab_size=11,pre_att_n=64,post_att_n=128):\n",
    "    \"\"\" Returns model object\n",
    "    args:\n",
    "        input_len: length of input\n",
    "        output_len: length of output\n",
    "        in_vocab_size: size of vocabulary for input\n",
    "        out_vocab_size: size of vocabulary for output\n",
    "        pre_att_n: pre-attention lstm number of hidden units\n",
    "        post_att_n: post-attention lstm number of hidden units\n",
    "    \"\"\"\n",
    "    X = Input(shape=(input_len,in_vocab_size),name=\"X\")\n",
    "    So = Input(shape=(post_att_n,),name=\"S\") # starting hidden state (zeros)\n",
    "    Co = Input(shape=(post_att_n,),name=\"C\") # starting cell state (zeros)\n",
    "    s = So\n",
    "    c = Co\n",
    "    \n",
    "    a = Bidirectional(LSTM(pre_att_n,activation=\"tanh\",return_sequences=True),merge_mode=\"concat\")(X)\n",
    "    outputs = []\n",
    "    \n",
    "    for _ in range(output_len):\n",
    "        context = one_step_attention(a,s)\n",
    "        s,_,c = post_activation_lstm_cell(context,initial_state=[s,c]) # one step with the post-activation lstm\n",
    "        out = output_layer(s) # linear layer, followed by softmax activation\n",
    "        outputs.append(out)\n",
    "        \n",
    "    model = Model(inputs=[X,So,Co],outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "X (InputLayer)                  [(None, 30, 37)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "S (InputLayer)                  [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 128)      52224       X[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 128)      0           S[0][0]                          \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[18][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 256)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[10][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[11][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[12][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[13][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[14][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[15][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[16][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[17][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[18][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[19][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30, 1)        257         concatenate_1[10][0]             \n",
      "                                                                 concatenate_1[11][0]             \n",
      "                                                                 concatenate_1[12][0]             \n",
      "                                                                 concatenate_1[13][0]             \n",
      "                                                                 concatenate_1[14][0]             \n",
      "                                                                 concatenate_1[15][0]             \n",
      "                                                                 concatenate_1[16][0]             \n",
      "                                                                 concatenate_1[17][0]             \n",
      "                                                                 concatenate_1[18][0]             \n",
      "                                                                 concatenate_1[19][0]             \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, 30, 1)        0           dense_2[10][0]                   \n",
      "                                                                 dense_2[11][0]                   \n",
      "                                                                 dense_2[12][0]                   \n",
      "                                                                 dense_2[13][0]                   \n",
      "                                                                 dense_2[14][0]                   \n",
      "                                                                 dense_2[15][0]                   \n",
      "                                                                 dense_2[16][0]                   \n",
      "                                                                 dense_2[17][0]                   \n",
      "                                                                 dense_2[18][0]                   \n",
      "                                                                 dense_2[19][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           softmax_1[10][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[11][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[12][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[13][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[14][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[15][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[16][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[17][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[18][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 softmax_1[19][0]                 \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "C (InputLayer)                  [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 128), (None, 131584      dot_1[10][0]                     \n",
      "                                                                 S[0][0]                          \n",
      "                                                                 C[0][0]                          \n",
      "                                                                 dot_1[11][0]                     \n",
      "                                                                 lstm_1[10][0]                    \n",
      "                                                                 lstm_1[10][2]                    \n",
      "                                                                 dot_1[12][0]                     \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[11][2]                    \n",
      "                                                                 dot_1[13][0]                     \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[12][2]                    \n",
      "                                                                 dot_1[14][0]                     \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[13][2]                    \n",
      "                                                                 dot_1[15][0]                     \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[14][2]                    \n",
      "                                                                 dot_1[16][0]                     \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[15][2]                    \n",
      "                                                                 dot_1[17][0]                     \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[16][2]                    \n",
      "                                                                 dot_1[18][0]                     \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[17][2]                    \n",
      "                                                                 dot_1[19][0]                     \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[18][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 11)           1419        lstm_1[10][0]                    \n",
      "                                                                 lstm_1[11][0]                    \n",
      "                                                                 lstm_1[12][0]                    \n",
      "                                                                 lstm_1[13][0]                    \n",
      "                                                                 lstm_1[14][0]                    \n",
      "                                                                 lstm_1[15][0]                    \n",
      "                                                                 lstm_1[16][0]                    \n",
      "                                                                 lstm_1[17][0]                    \n",
      "                                                                 lstm_1[18][0]                    \n",
      "                                                                 lstm_1[19][0]                    \n",
      "==================================================================================================\n",
      "Total params: 185,484\n",
      "Trainable params: 185,484\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn_model.compile(optimizer=Adam(lr=0.005),metrics=['accuracy'],loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((10000, 128)) # initialize cell states for all of the training examples\n",
    "c0 = np.zeros((10000, 128))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 25s 2ms/sample - loss: 13.0441 - dense_3_loss: 0.6191 - dense_3_1_loss: 0.6242 - dense_3_2_loss: 1.1944 - dense_3_3_loss: 2.1309 - dense_3_4_loss: 0.7547 - dense_3_5_loss: 1.0842 - dense_3_6_loss: 2.1155 - dense_3_7_loss: 0.8310 - dense_3_8_loss: 1.5017 - dense_3_9_loss: 2.1884 - dense_3_acc: 0.8226 - dense_3_1_acc: 0.7986 - dense_3_2_acc: 0.5741 - dense_3_3_acc: 0.2605 - dense_3_4_acc: 0.7981 - dense_3_5_acc: 0.5180 - dense_3_6_acc: 0.2216 - dense_3_7_acc: 0.7915 - dense_3_8_acc: 0.3509 - dense_3_9_acc: 0.2129\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 21s 2ms/sample - loss: 3.4496 - dense_3_loss: 0.0672 - dense_3_1_loss: 0.0569 - dense_3_2_loss: 0.3051 - dense_3_3_loss: 0.6965 - dense_3_4_loss: 0.0080 - dense_3_5_loss: 0.1195 - dense_3_6_loss: 0.6738 - dense_3_7_loss: 0.0107 - dense_3_8_loss: 0.5345 - dense_3_9_loss: 0.9773 - dense_3_acc: 0.9772 - dense_3_1_acc: 0.9776 - dense_3_2_acc: 0.8771 - dense_3_3_acc: 0.7528 - dense_3_4_acc: 0.9997 - dense_3_5_acc: 0.9652 - dense_3_6_acc: 0.7825 - dense_3_7_acc: 0.9994 - dense_3_8_acc: 0.7944 - dense_3_9_acc: 0.6309\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 21s 2ms/sample - loss: 1.2196 - dense_3_loss: 0.0217 - dense_3_1_loss: 0.0185 - dense_3_2_loss: 0.1349 - dense_3_3_loss: 0.1936 - dense_3_4_loss: 0.0030 - dense_3_5_loss: 0.0600 - dense_3_6_loss: 0.2375 - dense_3_7_loss: 0.0041 - dense_3_8_loss: 0.2273 - dense_3_9_loss: 0.3189 - dense_3_acc: 0.9926 - dense_3_1_acc: 0.9918 - dense_3_2_acc: 0.9471 - dense_3_3_acc: 0.9313 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 0.9791 - dense_3_6_acc: 0.9330 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 0.9160 - dense_3_9_acc: 0.8853\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 21s 2ms/sample - loss: 0.5932 - dense_3_loss: 0.0041 - dense_3_1_loss: 0.0015 - dense_3_2_loss: 0.0418 - dense_3_3_loss: 0.0756 - dense_3_4_loss: 0.0016 - dense_3_5_loss: 0.0423 - dense_3_6_loss: 0.1537 - dense_3_7_loss: 0.0018 - dense_3_8_loss: 0.1121 - dense_3_9_loss: 0.1588 - dense_3_acc: 1.0000 - dense_3_1_acc: 0.9999 - dense_3_2_acc: 0.9883 - dense_3_3_acc: 0.9857 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 0.9853 - dense_3_6_acc: 0.9519 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 0.9617 - dense_3_9_acc: 0.9479\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.3148 - dense_3_loss: 0.0020 - dense_3_1_loss: 6.1882e-04 - dense_3_2_loss: 0.0140 - dense_3_3_loss: 0.0368 - dense_3_4_loss: 9.5812e-04 - dense_3_5_loss: 0.0299 - dense_3_6_loss: 0.0999 - dense_3_7_loss: 0.0011 - dense_3_8_loss: 0.0547 - dense_3_9_loss: 0.0748 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 0.9977 - dense_3_3_acc: 0.9960 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 0.9885 - dense_3_6_acc: 0.9681 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 0.9831 - dense_3_9_acc: 0.9775\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 22s 2ms/sample - loss: 0.2006 - dense_3_loss: 0.0013 - dense_3_1_loss: 3.6619e-04 - dense_3_2_loss: 0.0067 - dense_3_3_loss: 0.0258 - dense_3_4_loss: 7.4599e-04 - dense_3_5_loss: 0.0236 - dense_3_6_loss: 0.0714 - dense_3_7_loss: 7.7437e-04 - dense_3_8_loss: 0.0254 - dense_3_9_loss: 0.0446 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 0.9996 - dense_3_3_acc: 0.9973 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 0.9905 - dense_3_6_acc: 0.9750 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 0.9933 - dense_3_9_acc: 0.9876\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.1277 - dense_3_loss: 8.3948e-04 - dense_3_1_loss: 2.1157e-04 - dense_3_2_loss: 0.0047 - dense_3_3_loss: 0.0196 - dense_3_4_loss: 4.8923e-04 - dense_3_5_loss: 0.0128 - dense_3_6_loss: 0.0486 - dense_3_7_loss: 5.5191e-04 - dense_3_8_loss: 0.0164 - dense_3_9_loss: 0.0234 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 0.9996 - dense_3_3_acc: 0.9975 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 0.9946 - dense_3_6_acc: 0.9843 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 0.9957 - dense_3_9_acc: 0.9937\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.1033 - dense_3_loss: 6.6627e-04 - dense_3_1_loss: 2.0201e-04 - dense_3_2_loss: 0.0046 - dense_3_3_loss: 0.0171 - dense_3_4_loss: 4.0491e-04 - dense_3_5_loss: 0.0090 - dense_3_6_loss: 0.0381 - dense_3_7_loss: 4.4229e-04 - dense_3_8_loss: 0.0124 - dense_3_9_loss: 0.0204 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 0.9996 - dense_3_3_acc: 0.9975 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 0.9965 - dense_3_6_acc: 0.9884 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 0.9970 - dense_3_9_acc: 0.9948\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.0597 - dense_3_loss: 4.4146e-04 - dense_3_1_loss: 1.4793e-04 - dense_3_2_loss: 0.0023 - dense_3_3_loss: 0.0137 - dense_3_4_loss: 2.7870e-04 - dense_3_5_loss: 0.0030 - dense_3_6_loss: 0.0213 - dense_3_7_loss: 3.0871e-04 - dense_3_8_loss: 0.0081 - dense_3_9_loss: 0.0102 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 0.9998 - dense_3_3_acc: 0.9978 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 0.9993 - dense_3_6_acc: 0.9943 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 0.9976 - dense_3_9_acc: 0.9978\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.0569 - dense_3_loss: 4.2981e-04 - dense_3_1_loss: 1.4972e-04 - dense_3_2_loss: 0.0022 - dense_3_3_loss: 0.0142 - dense_3_4_loss: 2.5218e-04 - dense_3_5_loss: 0.0032 - dense_3_6_loss: 0.0164 - dense_3_7_loss: 2.8876e-04 - dense_3_8_loss: 0.0080 - dense_3_9_loss: 0.0117 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 0.9998 - dense_3_3_acc: 0.9976 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 0.9997 - dense_3_6_acc: 0.9965 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 0.9982 - dense_3_9_acc: 0.9979\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 25s 2ms/sample - loss: 0.0247 - dense_3_loss: 2.8701e-04 - dense_3_1_loss: 8.7120e-05 - dense_3_2_loss: 0.0015 - dense_3_3_loss: 0.0094 - dense_3_4_loss: 1.9034e-04 - dense_3_5_loss: 9.5493e-04 - dense_3_6_loss: 0.0059 - dense_3_7_loss: 2.1746e-04 - dense_3_8_loss: 0.0021 - dense_3_9_loss: 0.0040 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 0.9999 - dense_3_3_acc: 0.9980 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 1.0000 - dense_3_6_acc: 0.9992 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 0.9999 - dense_3_9_acc: 0.9994\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 29s 3ms/sample - loss: 0.0143 - dense_3_loss: 2.0381e-04 - dense_3_1_loss: 6.8308e-05 - dense_3_2_loss: 8.0322e-04 - dense_3_3_loss: 0.0070 - dense_3_4_loss: 1.4558e-04 - dense_3_5_loss: 4.8267e-04 - dense_3_6_loss: 0.0026 - dense_3_7_loss: 1.5190e-04 - dense_3_8_loss: 0.0010 - dense_3_9_loss: 0.0018 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 1.0000 - dense_3_3_acc: 0.9980 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 1.0000 - dense_3_6_acc: 1.0000 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 1.0000 - dense_3_9_acc: 0.9999\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 21s 2ms/sample - loss: 0.0119 - dense_3_loss: 1.8738e-04 - dense_3_1_loss: 6.0206e-05 - dense_3_2_loss: 5.4765e-04 - dense_3_3_loss: 0.0068 - dense_3_4_loss: 1.4559e-04 - dense_3_5_loss: 3.4620e-04 - dense_3_6_loss: 0.0018 - dense_3_7_loss: 1.2791e-04 - dense_3_8_loss: 6.9645e-04 - dense_3_9_loss: 0.0011 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 1.0000 - dense_3_3_acc: 0.9978 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 1.0000 - dense_3_6_acc: 1.0000 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 1.0000 - dense_3_9_acc: 1.000017s - loss: 0.0156 - dense_3_loss: 2.0029e-04 - dense_3_1_loss: 6.4883e-05 - dense_3_2_loss: 5.6377e-04 - dense_3_3_loss: 0.0099 - dense_3_4_loss: 1.4356e-04 - dense_3_5_loss: 4.0771e-04 - dense_3_6_loss: 0.0021 - dense_3_7_loss: 1.3880e-04 - dense_3_8_loss: 7.3573e-04 - dense_3_9_loss: 0.0013 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 1.0000 - dense_3_3_acc: 0.9970 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 1.0000 \n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 23s 2ms/sample - loss: 0.0107 - dense_3_loss: 1.4680e-04 - dense_3_1_loss: 4.8382e-05 - dense_3_2_loss: 6.1703e-04 - dense_3_3_loss: 0.0049 - dense_3_4_loss: 1.1392e-04 - dense_3_5_loss: 5.4084e-04 - dense_3_6_loss: 0.0023 - dense_3_7_loss: 1.0576e-04 - dense_3_8_loss: 6.9482e-04 - dense_3_9_loss: 0.0012 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 1.0000 - dense_3_3_acc: 0.9988 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 0.9999 - dense_3_6_acc: 0.9998 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 1.0000 - dense_3_9_acc: 1.0000\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.0057 - dense_3_loss: 1.0327e-04 - dense_3_1_loss: 3.9483e-05 - dense_3_2_loss: 4.5522e-04 - dense_3_3_loss: 0.0025 - dense_3_4_loss: 9.3101e-05 - dense_3_5_loss: 2.1455e-04 - dense_3_6_loss: 9.9259e-04 - dense_3_7_loss: 8.0721e-05 - dense_3_8_loss: 4.2965e-04 - dense_3_9_loss: 7.4172e-04 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 1.0000 - dense_3_3_acc: 0.9994 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 1.0000 - dense_3_6_acc: 1.0000 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 1.0000 - dense_3_9_acc: 1.0000\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.0051 - dense_3_loss: 1.0511e-04 - dense_3_1_loss: 5.6466e-05 - dense_3_2_loss: 3.5376e-04 - dense_3_3_loss: 0.0025 - dense_3_4_loss: 7.3632e-05 - dense_3_5_loss: 1.5926e-04 - dense_3_6_loss: 8.4414e-04 - dense_3_7_loss: 6.7947e-05 - dense_3_8_loss: 3.6045e-04 - dense_3_9_loss: 6.0299e-04 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 1.0000 - dense_3_3_acc: 0.9992 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 1.0000 - dense_3_6_acc: 1.0000 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 1.0000 - dense_3_9_acc: 1.0000\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.0041 - dense_3_loss: 7.5406e-05 - dense_3_1_loss: 3.4370e-05 - dense_3_2_loss: 2.6796e-04 - dense_3_3_loss: 0.0018 - dense_3_4_loss: 7.0042e-05 - dense_3_5_loss: 1.5661e-04 - dense_3_6_loss: 7.4487e-04 - dense_3_7_loss: 6.4819e-05 - dense_3_8_loss: 3.2769e-04 - dense_3_9_loss: 5.1936e-04 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 1.0000 - dense_3_3_acc: 0.9995 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 1.0000 - dense_3_6_acc: 1.0000 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 1.0000 - dense_3_9_acc: 1.0000\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 22s 2ms/sample - loss: 0.0042 - dense_3_loss: 7.1216e-05 - dense_3_1_loss: 2.9858e-05 - dense_3_2_loss: 2.5835e-04 - dense_3_3_loss: 0.0021 - dense_3_4_loss: 7.1021e-05 - dense_3_5_loss: 1.3408e-04 - dense_3_6_loss: 7.0747e-04 - dense_3_7_loss: 6.0248e-05 - dense_3_8_loss: 3.1273e-04 - dense_3_9_loss: 4.9421e-04 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 1.0000 - dense_3_3_acc: 0.9995 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 1.0000 - dense_3_6_acc: 1.0000 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 1.0000 - dense_3_9_acc: 1.0000\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 22s 2ms/sample - loss: 0.0049 - dense_3_loss: 6.0577e-05 - dense_3_1_loss: 3.0808e-05 - dense_3_2_loss: 2.7469e-04 - dense_3_3_loss: 0.0024 - dense_3_4_loss: 5.1414e-05 - dense_3_5_loss: 1.2839e-04 - dense_3_6_loss: 0.0011 - dense_3_7_loss: 5.9959e-05 - dense_3_8_loss: 3.0074e-04 - dense_3_9_loss: 4.9041e-04 - dense_3_acc: 1.0000 - dense_3_1_acc: 1.0000 - dense_3_2_acc: 1.0000 - dense_3_3_acc: 0.9992 - dense_3_4_acc: 1.0000 - dense_3_5_acc: 1.0000 - dense_3_6_acc: 0.9999 - dense_3_7_acc: 1.0000 - dense_3_8_acc: 1.0000 - dense_3_9_acc: 1.0000\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 23s 2ms/sample - loss: 6.2067 - dense_3_loss: 0.0323 - dense_3_1_loss: 0.0675 - dense_3_2_loss: 0.3962 - dense_3_3_loss: 1.1584 - dense_3_4_loss: 0.2748 - dense_3_5_loss: 0.4197 - dense_3_6_loss: 1.2710 - dense_3_7_loss: 0.3512 - dense_3_8_loss: 0.9671 - dense_3_9_loss: 1.2686 - dense_3_acc: 0.9923 - dense_3_1_acc: 0.9843 - dense_3_2_acc: 0.8871 - dense_3_3_acc: 0.7095 - dense_3_4_acc: 0.9609 - dense_3_5_acc: 0.8908 - dense_3_6_acc: 0.6517 - dense_3_7_acc: 0.9225 - dense_3_8_acc: 0.7005 - dense_3_9_acc: 0.6394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x143448ba8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit([Xoh,s0,c0],outputs,epochs=20,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('feb 12 1970', '1970-02-12')\n"
     ]
    }
   ],
   "source": [
    "example_i = 2000\n",
    "print(dataset[example_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((1,128))\n",
    "c0 = np.zeros((1,128))\n",
    "example_x = Xoh[example_i]\n",
    "example_x = np.expand_dims(example_x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rnn_model.predict([example_x,s0,c0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(prediction,axis=-1) # getting index with the largest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [inv_machine_vocab[int(i)] for i in prediction] # turning prediction back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1970-02-22'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(output) # slight error with the encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized as global layers so their paramters are not re-initialized in model inner-loop\n",
    "softmax = Softmax()\n",
    "output_layer = Dense(11,activation='softmax')\n",
    "decoder_lstm_cell = LSTM(138,activation=\"tanh\",return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_len=30,output_len=10,in_vocab_size=37,out_vocab_size=11):\n",
    "    \"\"\" Returns model object, w/ comparable number of trainable parameters as the rnn w/ attention model\n",
    "    args:\n",
    "        input_len: length of input\n",
    "        output_len: length of output\n",
    "        in_vocab_size: size of vocabulary for input\n",
    "        out_vocab_size: size of vocabulary for output\n",
    "    \"\"\"\n",
    "    X = Input(shape=(input_len,in_vocab_size),name=\"X\")\n",
    "    in_0 = Input(shape=(1,11),name=\"in_0\") # first input vector to decoder network (zeros)\n",
    "    out = in_0\n",
    "    \n",
    "    # encoding the input into a 138x1 vector:\n",
    "    s,_,c = LSTM(138,activation=\"tanh\",return_sequences=False,return_state=True)(X) # encoder\n",
    "    outputs = []\n",
    "    \n",
    "    for _ in range(output_len):\n",
    "        s,_,c = decoder_lstm_cell(out,initial_state=[s,c])\n",
    "        out = output_layer(s) # linear layer, followed by softmax activation\n",
    "        outputs.append(out)\n",
    "        out = Reshape(target_shape=(1,11))(out) # serves as the next input to the decoder\n",
    "        \n",
    "    model = Model(inputs=[X,in_0],outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn_model.compile(optimizer=Adam(lr=0.005),metrics=['accuracy'],loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in0 = np.zeros((10000,1,11)) # initialize the first input to the decoder for all training data\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 14.6240 - dense_1_loss: 0.8427 - dense_1_1_loss: 0.8770 - dense_1_2_loss: 1.7897 - dense_1_3_loss: 2.4996 - dense_1_4_loss: 0.3843 - dense_1_5_loss: 1.0057 - dense_1_6_loss: 2.5400 - dense_1_7_loss: 0.6214 - dense_1_8_loss: 1.5906 - dense_1_9_loss: 2.4730 - dense_1_acc: 0.5631 - dense_1_1_acc: 0.5536 - dense_1_2_acc: 0.1982 - dense_1_3_acc: 0.0978 - dense_1_4_acc: 0.9392 - dense_1_5_acc: 0.5123 - dense_1_6_acc: 0.0992 - dense_1_7_acc: 0.9799 - dense_1_8_acc: 0.2213 - dense_1_9_acc: 0.0899\n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 13s 1ms/sample - loss: 11.2520 - dense_1_loss: 0.5025 - dense_1_1_loss: 0.4968 - dense_1_2_loss: 1.4672 - dense_1_3_loss: 2.3135 - dense_1_4_loss: 0.0240 - dense_1_5_loss: 0.5869 - dense_1_6_loss: 2.2725 - dense_1_7_loss: 0.0113 - dense_1_8_loss: 1.2624 - dense_1_9_loss: 2.3148 - dense_1_acc: 0.7476 - dense_1_1_acc: 0.7528 - dense_1_2_acc: 0.2915 - dense_1_3_acc: 0.1186 - dense_1_4_acc: 0.9926 - dense_1_5_acc: 0.7436 - dense_1_6_acc: 0.1690 - dense_1_7_acc: 0.9984 - dense_1_8_acc: 0.3333 - dense_1_9_acc: 0.1058\n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 13s 1ms/sample - loss: 9.6504 - dense_1_loss: 0.0935 - dense_1_1_loss: 0.0921 - dense_1_2_loss: 1.0348 - dense_1_3_loss: 2.0453 - dense_1_4_loss: 8.1860e-04 - dense_1_5_loss: 0.5689 - dense_1_6_loss: 2.2499 - dense_1_7_loss: 9.4358e-04 - dense_1_8_loss: 1.2546 - dense_1_9_loss: 2.3095 - dense_1_acc: 0.9681 - dense_1_1_acc: 0.9679 - dense_1_2_acc: 0.4465 - dense_1_3_acc: 0.2228 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 0.7469 - dense_1_6_acc: 0.1647 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.3421 - dense_1_9_acc: 0.1055\n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 22s 2ms/sample - loss: 7.8687 - dense_1_loss: 0.0279 - dense_1_1_loss: 0.0269 - dense_1_2_loss: 0.8507 - dense_1_3_loss: 1.2318 - dense_1_4_loss: 0.0018 - dense_1_5_loss: 0.3050 - dense_1_6_loss: 1.8949 - dense_1_7_loss: 0.0020 - dense_1_8_loss: 1.2370 - dense_1_9_loss: 2.2907 - dense_1_acc: 0.9915 - dense_1_1_acc: 0.9917 - dense_1_2_acc: 0.5901 - dense_1_3_acc: 0.5527 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 0.8752 - dense_1_6_acc: 0.2648 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.3712 - dense_1_9_acc: 0.1253\n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 5.0167 - dense_1_loss: 0.0048 - dense_1_1_loss: 0.0040 - dense_1_2_loss: 0.3741 - dense_1_3_loss: 0.3701 - dense_1_4_loss: 8.7184e-04 - dense_1_5_loss: 0.0976 - dense_1_6_loss: 0.9524 - dense_1_7_loss: 0.0020 - dense_1_8_loss: 1.0491 - dense_1_9_loss: 2.1618 - dense_1_acc: 0.9996 - dense_1_1_acc: 0.9997 - dense_1_2_acc: 0.8501 - dense_1_3_acc: 0.8924 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 0.9648 - dense_1_6_acc: 0.6577 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.5471 - dense_1_9_acc: 0.1923\n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 4.2104 - dense_1_loss: 0.0270 - dense_1_1_loss: 0.0250 - dense_1_2_loss: 0.2730 - dense_1_3_loss: 0.4811 - dense_1_4_loss: 0.0380 - dense_1_5_loss: 0.1087 - dense_1_6_loss: 0.5128 - dense_1_7_loss: 0.0543 - dense_1_8_loss: 0.8389 - dense_1_9_loss: 1.8517 - dense_1_acc: 0.9945 - dense_1_1_acc: 0.9935 - dense_1_2_acc: 0.8995 - dense_1_3_acc: 0.8614 - dense_1_4_acc: 0.9941 - dense_1_5_acc: 0.9700 - dense_1_6_acc: 0.8522 - dense_1_7_acc: 0.9931 - dense_1_8_acc: 0.6834 - dense_1_9_acc: 0.3084\n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 2.2163 - dense_1_loss: 0.0044 - dense_1_1_loss: 0.0038 - dense_1_2_loss: 0.0963 - dense_1_3_loss: 0.1302 - dense_1_4_loss: 3.2439e-04 - dense_1_5_loss: 0.0609 - dense_1_6_loss: 0.2545 - dense_1_7_loss: 0.0025 - dense_1_8_loss: 0.6023 - dense_1_9_loss: 1.0611 - dense_1_acc: 0.9993 - dense_1_1_acc: 0.9994 - dense_1_2_acc: 0.9770 - dense_1_3_acc: 0.9710 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 0.9815 - dense_1_6_acc: 0.9186 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.7895 - dense_1_9_acc: 0.5848\n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.2259 - dense_1_loss: 0.0018 - dense_1_1_loss: 0.0015 - dense_1_2_loss: 0.0520 - dense_1_3_loss: 0.0769 - dense_1_4_loss: 1.5660e-04 - dense_1_5_loss: 0.0410 - dense_1_6_loss: 0.1523 - dense_1_7_loss: 0.0017 - dense_1_8_loss: 0.4036 - dense_1_9_loss: 0.4949 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9879 - dense_1_3_acc: 0.9880 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 0.9882 - dense_1_6_acc: 0.9512 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.8677 - dense_1_9_acc: 0.8250\n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.6715 - dense_1_loss: 0.0014 - dense_1_1_loss: 9.1631e-04 - dense_1_2_loss: 0.0294 - dense_1_3_loss: 0.0502 - dense_1_4_loss: 1.2920e-04 - dense_1_5_loss: 0.0283 - dense_1_6_loss: 0.0960 - dense_1_7_loss: 0.0013 - dense_1_8_loss: 0.2425 - dense_1_9_loss: 0.2213 - dense_1_acc: 1.0000 - dense_1_1_acc: 0.9999 - dense_1_2_acc: 0.9933 - dense_1_3_acc: 0.9940 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 0.9913 - dense_1_6_acc: 0.9677 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.9181 - dense_1_9_acc: 0.9388\n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.3621 - dense_1_loss: 7.8302e-04 - dense_1_1_loss: 5.9491e-04 - dense_1_2_loss: 0.0178 - dense_1_3_loss: 0.0363 - dense_1_4_loss: 8.7890e-05 - dense_1_5_loss: 0.0164 - dense_1_6_loss: 0.0507 - dense_1_7_loss: 0.0010 - dense_1_8_loss: 0.1338 - dense_1_9_loss: 0.1046 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9953 - dense_1_3_acc: 0.9964 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 0.9956 - dense_1_6_acc: 0.9853 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.9567 - dense_1_9_acc: 0.9716\n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 0.1649 - dense_1_loss: 6.5988e-04 - dense_1_1_loss: 4.8859e-04 - dense_1_2_loss: 0.0126 - dense_1_3_loss: 0.0308 - dense_1_4_loss: 8.7326e-05 - dense_1_5_loss: 0.0065 - dense_1_6_loss: 0.0233 - dense_1_7_loss: 6.9596e-04 - dense_1_8_loss: 0.0524 - dense_1_9_loss: 0.0375 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9976 - dense_1_3_acc: 0.9967 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 0.9989 - dense_1_6_acc: 0.9943 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.9876 - dense_1_9_acc: 0.9931\n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.1246 - dense_1_loss: 4.2631e-04 - dense_1_1_loss: 3.6122e-04 - dense_1_2_loss: 0.0074 - dense_1_3_loss: 0.0250 - dense_1_4_loss: 6.3592e-05 - dense_1_5_loss: 0.0024 - dense_1_6_loss: 0.0122 - dense_1_7_loss: 4.6552e-04 - dense_1_8_loss: 0.0316 - dense_1_9_loss: 0.0447 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9995 - dense_1_3_acc: 0.9972 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 0.9998 - dense_1_6_acc: 0.9981 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.9931 - dense_1_9_acc: 0.9875\n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1405 - dense_1_loss: 4.4446e-04 - dense_1_1_loss: 3.1423e-04 - dense_1_2_loss: 0.0064 - dense_1_3_loss: 0.0256 - dense_1_4_loss: 6.8116e-05 - dense_1_5_loss: 0.0080 - dense_1_6_loss: 0.0257 - dense_1_7_loss: 4.3512e-04 - dense_1_8_loss: 0.0319 - dense_1_9_loss: 0.0416 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9996 - dense_1_3_acc: 0.9973 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 0.9977 - dense_1_6_acc: 0.9941 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.9923 - dense_1_9_acc: 0.9880\n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 13s 1ms/sample - loss: 0.0437 - dense_1_loss: 3.0423e-04 - dense_1_1_loss: 2.0633e-04 - dense_1_2_loss: 0.0039 - dense_1_3_loss: 0.0210 - dense_1_4_loss: 5.4277e-05 - dense_1_5_loss: 7.9199e-04 - dense_1_6_loss: 0.0040 - dense_1_7_loss: 2.6805e-04 - dense_1_8_loss: 0.0077 - dense_1_9_loss: 0.0055 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9998 - dense_1_3_acc: 0.9974 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 1.0000 - dense_1_6_acc: 0.9998 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.9996 - dense_1_9_acc: 0.9996\n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 13s 1ms/sample - loss: 0.0316 - dense_1_loss: 2.3274e-04 - dense_1_1_loss: 1.5584e-04 - dense_1_2_loss: 0.0036 - dense_1_3_loss: 0.0182 - dense_1_4_loss: 4.0368e-05 - dense_1_5_loss: 4.9888e-04 - dense_1_6_loss: 0.0021 - dense_1_7_loss: 1.9109e-04 - dense_1_8_loss: 0.0037 - dense_1_9_loss: 0.0029 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9997 - dense_1_3_acc: 0.9975 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 1.0000 - dense_1_6_acc: 1.0000 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 0.9998 - dense_1_9_acc: 0.9999\n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 13s 1ms/sample - loss: 0.0272 - dense_1_loss: 1.8828e-04 - dense_1_1_loss: 1.4189e-04 - dense_1_2_loss: 0.0027 - dense_1_3_loss: 0.0174 - dense_1_4_loss: 3.5790e-05 - dense_1_5_loss: 3.6986e-04 - dense_1_6_loss: 0.0017 - dense_1_7_loss: 1.5271e-04 - dense_1_8_loss: 0.0025 - dense_1_9_loss: 0.0021 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9998 - dense_1_3_acc: 0.9973 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 1.0000 - dense_1_6_acc: 1.0000 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 1.0000 - dense_1_9_acc: 1.0000\n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.0235 - dense_1_loss: 1.6594e-04 - dense_1_1_loss: 1.2719e-04 - dense_1_2_loss: 0.0024 - dense_1_3_loss: 0.0153 - dense_1_4_loss: 2.9020e-05 - dense_1_5_loss: 3.0734e-04 - dense_1_6_loss: 0.0014 - dense_1_7_loss: 1.2636e-04 - dense_1_8_loss: 0.0019 - dense_1_9_loss: 0.0017 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9998 - dense_1_3_acc: 0.9975 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 1.0000 - dense_1_6_acc: 1.0000 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 1.0000 - dense_1_9_acc: 1.0000\n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.0221 - dense_1_loss: 1.3134e-04 - dense_1_1_loss: 1.1437e-04 - dense_1_2_loss: 0.0022 - dense_1_3_loss: 0.0150 - dense_1_4_loss: 2.5761e-05 - dense_1_5_loss: 2.5692e-04 - dense_1_6_loss: 0.0012 - dense_1_7_loss: 1.0730e-04 - dense_1_8_loss: 0.0016 - dense_1_9_loss: 0.0014 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9999 - dense_1_3_acc: 0.9975 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 1.0000 - dense_1_6_acc: 1.0000 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 1.0000 - dense_1_9_acc: 1.0000\n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.0210 - dense_1_loss: 1.2603e-04 - dense_1_1_loss: 1.0784e-04 - dense_1_2_loss: 0.0026 - dense_1_3_loss: 0.0140 - dense_1_4_loss: 2.2425e-05 - dense_1_5_loss: 2.4151e-04 - dense_1_6_loss: 0.0011 - dense_1_7_loss: 9.6834e-05 - dense_1_8_loss: 0.0014 - dense_1_9_loss: 0.0013 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9998 - dense_1_3_acc: 0.9974 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 1.0000 - dense_1_6_acc: 1.0000 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 1.0000 - dense_1_9_acc: 1.0000\n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 13s 1ms/sample - loss: 0.0176 - dense_1_loss: 1.1390e-04 - dense_1_1_loss: 9.3101e-05 - dense_1_2_loss: 0.0019 - dense_1_3_loss: 0.0120 - dense_1_4_loss: 1.8973e-05 - dense_1_5_loss: 2.0121e-04 - dense_1_6_loss: 9.1024e-04 - dense_1_7_loss: 8.7678e-05 - dense_1_8_loss: 0.0012 - dense_1_9_loss: 0.0012 - dense_1_acc: 1.0000 - dense_1_1_acc: 1.0000 - dense_1_2_acc: 0.9999 - dense_1_3_acc: 0.9976 - dense_1_4_acc: 1.0000 - dense_1_5_acc: 1.0000 - dense_1_6_acc: 1.0000 - dense_1_7_acc: 1.0000 - dense_1_8_acc: 1.0000 - dense_1_9_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13e868748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit([Xoh,in0],outputs,epochs=20,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('feb 12 1970', '1970-02-12')\n"
     ]
    }
   ],
   "source": [
    "example_i = 2000\n",
    "print(dataset[example_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in0 = np.zeros((1,1,11))\n",
    "example_x = Xoh[example_i]\n",
    "example_x = np.expand_dims(example_x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rnn_model.predict([example_x,in0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(prediction,axis=-1) # getting index with the largest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [inv_machine_vocab[int(i)] for i in prediction] # turning prediction back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1970-02-12'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
