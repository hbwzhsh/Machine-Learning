{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing RNN with Attention for the task of date normalization\n",
    "\n",
    "The technique used is outlined in the paper - <i>Neural Machine Translation by Jointly Learning to Align and Translate</i>. This specific project is a component of the deeplearning.ai deep learning course, and the helper functions used to create the fake date data was ported from their tools (as well as much of the architecture). Here, the model is tasked to translate dates such as \"25th of october 1990\" to \"1990-10-25\". The output of this model is of fixed length, so there is no necessity for there to be a token indicating the end of the encoding. In addition, I built out a standard encoder-decoder network in order to compare performances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import RepeatVector,LSTM,Bidirectional,Dense,ReLU,Softmax\n",
    "from tensorflow.keras.layers import Input,Concatenate,Dot,Activation,Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.insert(1,'../helpers/')\n",
    "from nmt_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 30, 37) (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "# human_vocab is characters, numbers, and certain symbols\n",
    "# machine_vocab is numbers, and the \"-\" symbol\n",
    "# inv_machine_vocab is translation of model prediction argmax to character\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m=10000)\n",
    "X,Y,Xoh,Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx=30, Ty=10) # output is len 10. assume max input length is 30\n",
    "print(Xoh.shape,Yoh.shape) # one-hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3] # (human_input, machine_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run keras functions with numpy data\n",
    "# sess = tf.InteractiveSession()\n",
    "# a = np.array([1.0,2,3,4,5])\n",
    "# out = softmax(a)\n",
    "# out.eval()\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN With Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized as global layers so their paramters are not re-initialized in model inner-loop\n",
    "repeat = RepeatVector(30) # the max-size of the input, function to be applied\n",
    "concatenate = Concatenate()\n",
    "dense_layer = Dense(1,activation = \"relu\") # single layered DNN, for calculating attention weights\n",
    "softmax = Softmax(axis=1) # is not axis=1, then softmax won't work (b/c shape is (None,30,1))\n",
    "dot_product = Dot(axes=1)\n",
    "post_activation_lstm_cell = LSTM(128,activation=\"tanh\",return_state=True) # returns last state AND output\n",
    "output_layer = Dense(11,activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_step_attention(a,s):\n",
    "    \"\"\" Calculates the context for this timestep\n",
    "    args:\n",
    "        a: output from bidirectional rnn\n",
    "        s: rnn state at the previous timestep; (,128)\n",
    "    \"\"\"\n",
    "    s = repeat(s) # (30,128)\n",
    "    concat = concatenate([a,s]) # (30,256)\n",
    "    e = dense_layer(concat) # (30,1), calculating the energies which represent the unbounded attention weights\n",
    "    att = softmax(e) # (30,1), scaling the attention weights\n",
    "    context = dot_product([att,a]) # (1,128), calculating the input context vector for post-attention lstm\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_len=30,output_len=10,in_vocab_size=37,out_vocab_size=11,pre_att_n=64,post_att_n=128):\n",
    "    \"\"\" Returns model object\n",
    "    args:\n",
    "        input_len: length of input\n",
    "        output_len: length of output\n",
    "        in_vocab_size: size of vocabulary for input\n",
    "        out_vocab_size: size of vocabulary for output\n",
    "        pre_att_n: pre-attention lstm number of hidden units\n",
    "        post_att_n: post-attention lstm number of hidden units\n",
    "    \"\"\"\n",
    "    X = Input(shape=(input_len,in_vocab_size),name=\"X\")\n",
    "    So = Input(shape=(post_att_n,),name=\"S\") # starting hidden state (zeros)\n",
    "    Co = Input(shape=(post_att_n,),name=\"C\") # starting cell state (zeros)\n",
    "    s = So\n",
    "    c = Co\n",
    "    \n",
    "    a = Bidirectional(LSTM(pre_att_n,activation=\"tanh\",return_sequences=True),merge_mode=\"concat\")(X)\n",
    "    outputs = []\n",
    "    \n",
    "    for _ in range(output_len):\n",
    "        context = one_step_attention(a,s)\n",
    "        s,_,c = post_activation_lstm_cell(context,initial_state=[s,c]) # one step with the post-activation lstm\n",
    "        out = output_layer(s) # linear layer, followed by softmax activation\n",
    "        outputs.append(out)\n",
    "        \n",
    "    model = Model(inputs=[X,So,Co],outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn_model.compile(optimizer=Adam(lr=0.005),metrics=['accuracy'],loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((10000, 128)) # initialize cell states for all of the training examples\n",
    "c0 = np.zeros((10000, 128))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.fit([Xoh,s0,c0],outputs,epochs=20,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('feb 12 1970', '1970-02-12')\n"
     ]
    }
   ],
   "source": [
    "example_i = 2000\n",
    "print(dataset[example_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((1,128))\n",
    "c0 = np.zeros((1,128))\n",
    "example_x = Xoh[example_i]\n",
    "example_x = np.expand_dims(example_x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rnn_model.predict([example_x,s0,c0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(prediction,axis=-1) # getting index with the largest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [inv_machine_vocab[int(i)] for i in prediction] # turning prediction back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1970-02-22'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(output) # slight error with the encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized as global layers so their paramters are not re-initialized in model inner-loop\n",
    "softmax = Softmax()\n",
    "output_layer = Dense(11,activation='softmax')\n",
    "decoder_lstm_cell = LSTM(138,activation=\"tanh\",return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_len=30,output_len=10,in_vocab_size=37,out_vocab_size=11):\n",
    "    \"\"\" Returns model object, w/ comparable number of trainable parameters as the rnn w/ attention model\n",
    "    args:\n",
    "        input_len: length of input\n",
    "        output_len: length of output\n",
    "        in_vocab_size: size of vocabulary for input\n",
    "        out_vocab_size: size of vocabulary for output\n",
    "    \"\"\"\n",
    "    X = Input(shape=(input_len,in_vocab_size),name=\"X\")\n",
    "    in_0 = Input(shape=(1,11),name=\"in_0\") # first input vector to decoder network (zeros)\n",
    "    out = in_0\n",
    "    \n",
    "    # encoding the input into a 138x1 vector:\n",
    "    s,_,c = LSTM(138,activation=\"tanh\",return_sequences=False,return_state=True)(X) # encoder\n",
    "    outputs = []\n",
    "    \n",
    "    for _ in range(output_len):\n",
    "        s,_,c = decoder_lstm_cell(out,initial_state=[s,c])\n",
    "        out = output_layer(s) # linear layer, followed by softmax activation\n",
    "        outputs.append(out)\n",
    "        out = Reshape(target_shape=(1,11))(out) # serves as the next input to the decoder\n",
    "        \n",
    "    model = Model(inputs=[X,in_0],outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn_model.compile(optimizer=Adam(lr=0.005),metrics=['accuracy'],loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "in0 = np.zeros((10000,1,11)) # initialize the first input to the decoder for all training data\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.fit([Xoh,in0],outputs,epochs=20,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('feb 12 1970', '1970-02-12')\n"
     ]
    }
   ],
   "source": [
    "example_i = 2000\n",
    "print(dataset[example_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in0 = np.zeros((1,1,11))\n",
    "example_x = Xoh[example_i]\n",
    "example_x = np.expand_dims(example_x,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rnn_model.predict([example_x,in0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(prediction,axis=-1) # getting index with the largest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [inv_machine_vocab[int(i)] for i in prediction] # turning prediction back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1970-02-12'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
